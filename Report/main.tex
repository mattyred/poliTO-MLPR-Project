%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Article
% LaTeX Template
% Version 2.0 (28/2/17)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[10pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)
\usepackage{placeins}
\usepackage{multirow}
\usepackage{amsmath}
\newcommand{\comment}[1]{}
\input{structure.tex} % Specifies the document structure and loads requires packages

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{MLPR Exam Project: \\Gender Detection} % The article title

\author{
	\authorstyle{Mattia Rosso [s294711]} % Author
	%\newline\newline % Space before institutions
	%\textsuperscript{1}\institution{Universidad Nacional Autónoma de México, Mexico City, Mexico}\\ % Institution 1
	%\textsuperscript{2}\institution{University of Texas at Austin, Texas, United States of America}\\ % Institution 2
	%\textsuperscript{3}\institution{\texttt{LaTeXTemplates.com}} % Institution 3
}

% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}

\date{\today} % Add a date here if you would like one to appear underneath the title block, use \today for the current date, leave empty for no date

%----------------------------------------------------------------------------------------

\begin{document}
\maketitle % Print the title
\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{This project is inteded to show a binary classification 
task on a datased made of 12 continuous observations coming from speking embeddings. 
A speaker embedding represents a smal-dimensional, fixed size representation of an utterance.
Features can be seen as points in the m-dimensional embedding space (and the embeddings
have already been computed). This is a task where classes are balanced both in training and
evaluation set}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Dataset analysis}
\subsection{Training and evaluation sets}
The training set contains:
\begin{itemize}
	\item Training Set: 3000 samples belonging to Male class (Label = 0) and
						3000 samples belonging to Female class (Label = 1).
	\item Evaluation Set: 2000 samples belonging to Male class (Label = 0) and
						  2000 samples belonging to Female class (Label = 1).
\end{itemize}

\subsection{Training set features analysis}
\subsection{Features Statistics}
All the features are contiguous and their main statics can be
showed through a boxplot in figure \ref{boxplot}. 

\subsection{Z-normalization}
A useful operation that can be applied in order to avoid to deal with numerical 
issues and to make data more uniform is to apply Z-normalization as a preprocessing step:\\
\begin{center}
	\begin{math}
		z_{i,j} = \frac{x_{i,j} - \mu_{j}}{\sigma_{j}} \forall x_{i} \in D 
	\end{math}
\end{center}
Where $z_{i,j}$ is the Z-normalized value correspongin to the feature $j$
of sample $i$ while $\mu_{j}$ and $\sigma_{j}$ are, respectively,
the mean and the variance computed over all values for feature $j$.
\comment{
\FloatBarrier
	\begin{table}
		\caption{Features Statistics}
		\centering
		\begin{tabular}{ |l|l|l|l|l| }
			\hline
			\multicolumn{5}{ |c| }{Statistics} \\
			\hline
			Feature & Min & Max & Mean & StdDev \\ \hline
			\multirow{2}{*}{0}
			 & 0 & 0 & 0 & 0 \\
			 & 0 & 0 & 0 & 0  \\ \hline
			\multirow{2}{*}{1}
			 & 0 & 0 & 0 & 0  \\
			 & 0 & 0 & 0 & 0  \\ \hline
			\multirow{2}{*}{2}
			& 0 & 0 & 0 & 0  \\
			& 0 & 0 & 0 & 0  \\ \hline
		\end{tabular}
	\end{table}
\FloatBarrier
}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/boxplot.png}
	\caption{Features boxplot}
	\label{boxplot} 
\end{figure}

\subsection{Features distribution}
By plotting historams for each feature, separated for male and female, it is possible to show
how much each feature follows a gaussian distribution in order to understend whether a pre processing
like gaussianization can be useful to go on with our classification task
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/hist_znorm.png}
	\caption{Z-normalized features distribution}
	\label{hist_znorm} 
\end{figure}

We can notice from figure \ref{hist_znorm} that almost all the features are already 
well-distributed except for features 3, 7, 9. We can apply an additional pre-processing step in
order to make all the features following a gaussian distribution.

\subsubsection{Gaussianization}
Gaussianization is a pre processing step that maps each fature to values whose empirical cumulative
distribution is well approximated by a Gaussian cumulative distribution function. For each
feature $x$ that we want to gaussianize we firstly compute the rank over the dataset:\\
\begin{center}
	\begin{math}
		r(x) = \frac{\sum_{i=1}^{N}\mathbb{I}[x_{i} < x] + 1}{N + 2}
	\end{math}
\end{center}
where $\mathbb{I}$ is the indicator function ($1$ when the condition inside $[ ]$ is true, $0$ 
otherwise). Actually, we are counting how many samples in the dataset $D$ have a greater value
with respect to the feature we are computing the rank on. \\
The next step is to compute the transformed feature as $y = \Phi^{-1}(r(x))$ where $\Phi$ is
the inverse of the cumulative distribution function. 

\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/hist_gau.png}
	\caption{Gaussianized features distribution}
	\label{hist_gau} 
\end{figure}

\subsection{Features correlation}
We can show how much features are correlated by using a heatmap plot showing a darker
color inside cells $[i, j]$ for which it exists an high correlation among feaure $i$ and 
feature $j$. We are going to use the Pearson correlation coefficient to compute the
correlation among feature $X$ and feature $Y$:
\begin{center}
	\begin{math}
		|\frac{Cov(X, Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}|
	\end{math}
\end{center}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/heatmap_znormgau.png}
	\caption{Z-normalized+Gaussianized features correlation: grey the whole dataset, orange F class, 
	blue M class}
	\label{heatmap} 
\end{figure}

Correlation is quite high among most of the features, we can understand that applying PCA
would be meaningful for values of $m$ not below $10$ or $9$ at most. For smaller values of $m$
we would lose important information coming from high-correlated features. 

\section{Dimensionality reduction}
Before proceeding with the classification task i would spend some words on the possible 
dimensioanlity reduction rechniques that can be applied: PCA, LDA.
\subsection{PCA}
As already anticipated, given the heatmap in figure \ref{heatmap} we can observe that PCA
with reasonable values of $m$ can be applied. PCA is a dimensionality reduction technique
that, given a centered dataset $X = {x_{1}, ..., x_{k}}$, it aims to find the subspace of 
$\mathbb{R}^{n}$ that allows to preserve most of the information (the directions with the
highest variance).\\Starting from the sample covariance matrix
\begin{center}
	\begin{math}
		C = \frac{1}{K} \sum_{i}^{}(x_{i} - \bar{x})(x_{i}-\bar{x})^T
	\end{math}
\end{center}
we compute the eigen-decomposition of $C = U\Sigma U^T$ and project the data in the subspace
spanned by the $m$ columns of $U$ corresponding to the $m$ highest eigenvalues:
\begin{center}
	\begin{math}
		y_{i} = P^T(x_{i}-\bar{x})
	\end{math}
\end{center}
In order to select the optimal $m$ we can use a cross-validation approach by inspecting how much
of the total variance of data we are able to retain by using that value of $m$. We exploit the fact
that each eigenvalue correspond to the variance along the corresponding axis and the
eigenvalues are the elements of the diagonal of the matrix $\Sigma$. We selecte $m$ as:
\begin{center}
	\begin{math}
		\min_m\;s.t \frac{\sum_{i}^{m}\sigma_{i}}{\sum_{i}^{n}\sigma_{i}} >= t,\;t \ge 95\%
	\end{math}
\end{center}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/pca.png}
	\caption{3-fold cross validation for PCA impact evaluation}
	\label{pca} 
\end{figure}
We can clearly understand from figure \ref{pca} that values of $m < 9$ would rapidly
decrease the amout of retained variance. We will see better later on how this would 
badly impact the performance of the classifiers

\subsection{LDA}
PCA technique is unsupervised so we have no guarantee of obtainig discriminant directions.
Despite the fact that LDA allows to find at most $C - 1$, where $C$ is the number of classes,
directions, so it makes no sense to apply it as a dimensionality reduction technique, it can
be used as a linear classifier and we can understand it by its definition; LDA mazimizes the
between-class variability over the within-class variability ratio for the transformed samples:
\begin{center}
	\begin{math}
		\mathcal{L}(w) = \frac{s_{B}}{s_{W}} = \max_w \frac{w^TS_{B}w}{w^TS_{W}w}
	\end{math}
\end{center}
It can be proved that optimal solution correspond to the eigenvector of $S_{W}^-1S_{B}$ 
corresponding to the largest eigenvalue. Once that we have estimated $w$ we can project
the test samples over $w$ and assign the class label looking at the score obtained:\\
\begin{center}
	\begin{math}
		f(x)=\left\{
		\begin{array}{ll}
			1, & \mbox{if $x<0$}.\\
			0, & \mbox{otherwise}.
		\end{array}
		\right.
	\end{math}
\end{center}
It will be showed later the result of the classifier
\section{Classification models analysis}
\subsection{Premises}
In the next paragraphs we are going to compare different classification models. 
We will employ a k-fold cross validation technique (with $k=3$) for model evaluation and the
best models will be chosen to train the entire training set for performing a final comparison.
We will consider three types of applications:
\begin{center}
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.1, 1, 1)$\\
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.5, 1, 1)$\\
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.9, 1, 1)$\\
\end{center}
and the target application will be 
\begin{center}
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.5, 1, 1)$
\end{center}
We are interested
in selecting the most promising approach and we will infact perform measures in term of minimum
detection cost:
\begin{center}
	\begin{math}
		DCF = \frac{DCF_{u}(\pi_{T}, C_{fn}, C_{fp})}{min(\pi_{T},C_{fn}, (1-\pi_{T}C_{fp}))} = 
			\frac{\pi_{T}C_{fn}P_{fn} + (1-\pi_{T})C_{fp}P_{fp}}{min(\pi_{T},C_{fn}, (1-\pi_{T}C_{fp}))}
	\end{math}
\end{center}
and for $minDCF$ computation we will look for:
\begin{center}
	\begin{math}
		t' = -log(\frac{\tilde{\pi}}{1-\tilde{\pi}})
	\end{math}
\end{center}
that allows us to obtain the lowest possible $DCF$ (as if knew in advance this optimal threshold)
\subsection{Gaussian models}
The first class of models we are going to analyze are the generative gaussian models. Model assumption are that,
given the dataset $X$ we assume that the sample $x_{t}$ is a realization of the R.V. $X_{t}$. A simple
model consists in assuming that our data, given the class, can be described by a Gaussian distribution:
\begin{center}
	\begin{math}
		(X_{t}|C_{t}=c) \sim (X|C=c) \sim \mathcal{N} (x_{t}|\mu_{c},\Sigma_{c})
	\end{math}
\end{center}
We will assign a probabilistic score to each sample in term of the class-posterior 
log-likelihood ratio:
\begin{center}
	\begin{math}
		log\;r(x_{t}) = log\;\frac{P(C = h_{1}|x_{t})}{P(C = h_{0}|x_{t})}
	\end{math}
\end{center}
We can expand this expression by writing:
\begin{center}
	\begin{math}
		log\;r(x_{t}) = log\;\frac{f_{X|C}(x_{t}|h_{1})}{f_{X|C}(x_{t}|h_{0})} + log\;\frac{\pi}{1-\pi}
	\end{math}
\end{center}
While the training phase consists in estimating the model parameters the scoring phase
consists in computing the log-likelihood ratio (first term of the equation) for each sample. 
It will be then compared with  a threshold specific for each application for computing 
the $minDCF$. What differentiate the different Gaussian models is the way how we estimate the
model parameters.
\subsubsection{MVG Gaussian Classifier}
The ML solution to the previous desribed problem is given by the empirical mean and covariance
matrix for each class:
\begin{center}
	$\mu_{c}^* = \frac{1}{N_c}\sum_{i=1}^{N}x_{c,i}$ \\
	$\Sigma_c^* = \frac{1}{N_c}\sum_{i=1}^{N}(x_{c,i}-\mu_c^*)(x_{c,i}-\mu_c^*)$
\end{center}
We will then compute the log densities for each sample by using the estimated model parameters
\subsubsection{Naive Bayes Classifier}
The Naive Bayes assumption simplifies the MVG full covariance model stating that if we knew
that for each class the componenets are approximately independent we can assume that the 
distribution $X|C$ can be factorized over its components. The ML solution to this problem is:
\begin{center}
	$\mu_{c,[j]}^* = \frac{1}{N_c}\sum_{i|c_i=c}^{}x_{i,[j]}$ \\
	$\sigma_{c,[j]}^2 = \frac{1}{N_c}\sum_{i|c_i=c}^{}(x_{i,[j]}-\mu_{c,[j]})^2$
\end{center}
The density of a sample $x$ can be expressed as $\mathcal{N}(x|\mu_c,\Sigma_c)$ where
$\mu_c$ is an array where each element $\mu_{c,[j]}$ is the the mean for each class
for each component while $\Sigma_c$ is a diagonal covariance matrix. The Naive Bayes classifier
corresponds to the MVG full covariance classifier with a diagonal covariance matrix
\subsubsection{Tied Gaussian Classifier}
This model assumes that the covariance matrices of the different class are tied (we consider
only one covariance matrix common to all classes). We are assuming that:
\begin{center}
	$f_{X|C}(x|c) = \mathcal{N} (x|\mu_c, \Sigma)$
\end{center}
so each class has its own mean but the covariance matrix is the same for all the classes. The ML
solution to this problem is:
\begin{center}
	$\mu_{c}^* = \frac{1}{N_c}\sum_{i=1}^{N}x_{c,i}$ \\
	$\Sigma^* = \frac{1}{N}\sum_c{}^{}\sum_{i|c_i=c}^{}(x_{i}-\mu_c^*)(x_{i}-\mu_c^*)$
\end{center}
This model is strongly related to LDA (used as a linear classfication model). By considering 
the binary log-likelihood ratio of the tied model we obtain a linear decision function:
\begin{center}
	$llr(x) = log\;\frac{f_{X|C}(x|h_1)}{f_{X|C}(x|h_0)} = x^Tb + c$ 
\end{center}
where $b$ and $c$ are functions of class means and (tied) covariance matrix. On the other hand,
projecting over the LDA subspace is, up to a scaling factor $k$, given by:
\begin{center}
	$w^Tx = k \cdot x^T\varLambda (\mu_1-\mu_0)$ 
\end{center}
where $\varLambda (\mu_1-\mu_0) = b$. The LDA assumption that all the classes have the same
within class covariance is related to the assumption done for the tied model.\
\subsubsection{Gaussian Models Comparison}
\FloatBarrier
	\begin{table}
		\caption{Gaussian Models}
		\centering
		\begin{tabular}{ |l|l|l|l| }
			\hline
			& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.128 & 0.048 & 0.125\\
			 Tied Cov & 0.122 & \textcolor{red}{0.046} & 0.127\\
			 Naive Bayes & 0.822 & 0.567 & \textcolor{blue}{0.856}\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=11)}} \\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.265 & 0.100 & 0.231\\
			 Tied Cov & 0.257 & \textcolor{red}{0.098} & 0.227\\
			 Naive Bayes & \textcolor{orange}{0.278} & \textcolor{orange}{0.108} & \textcolor{orange}{0.245}\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.303 & 0.115 & 0.267\\
			 Tied Cov & 0.293 & \textcolor{red}{0.112} & 0.264\\
			 Naive Bayes & \textcolor{orange}{0.306} & \textcolor{orange}{0.121} & \textcolor{orange}{0.283}\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - no PCA}} \\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.218 & \textcolor{red}{0.078} & 0.191\\
			 Tied Cov & 0.208 & \textcolor{red}{0.078} & 0.189\\
			 Naive Bayes & 0.813 & 0.586 & \textcolor{blue}{0.847} \\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=11)}} \\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.227 & 0.087 & 0.218\\
			 Tied Cov & 0.215 & \textcolor{red}{0.084} & 0.208\\
			 Naive Bayes & \textcolor{blue}{0.278} & \textcolor{orange}{0.106} & 0.257\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=10)}} \\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.223 & 0.084 & 0.211\\
			 Tied Cov & 0.212 & \textcolor{red}{0.082} & 0.207\\
			 Naive Bayes & \textcolor{blue}{0.279} & \textcolor{orange}{0.103} & 0.254\\
			\hline
		\end{tabular}
	\end{table}
\FloatBarrier

A graphical version of the table can be helpful in analyzing results:
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/gaumodels.png}
	\caption{Top: Z-Normalized feautres, Bottom: Gaussianized features}
	\label{gauplots} 
\end{figure}

Some observations:
\begin{itemize}
	\item We can notice that Full-Cov model and Tied-Cov model achieve very good and similar results with
		  slighlty better performances for the tied model.
	\item Gaussianization pre processing does't really help in achieving better results maybe
		  because data are already well distributed according to the Gaussian assumptions
	\item Naive Bayes assumption doesn't hold really well, in particular if PCA is not applied. When 
		  PCA is applied it has a really good impact only on Naive Bayes model and especialliy if also
		  combined with Gaussianization pre processing.
	\item Regarding Full and Tied models with PCA(m=11) there is no a high performance degradation
		  since the minDCF obtained are still good. For lower values of $m$ the models become less
		  able in taking decisions and the minDCF increases
	\item For the MVG classifiers best performances are achieved by the Tied-Cov classifier with
		  only Z-normalization pre processing and withouth PCA. Really good performacnes are also
		  achieved by the Tied-Cov model trained with Gaussianized
		  fetures and no-PCA (the PCA(m=11) version of this model achieves worse but comparable result w.r.t. to the no-PCA version
		  and it can be selected to try to better avoid overfitting by reducing the features 
		  space and also for reducing computational effort)
\end{itemize}
\underline{Best Gaussian Models}: 
\begin{itemize}
	\item Tied Cov (Z-Normalization, no PCA)
	\item Tied Cov (Gaussianization, PCA(m=11))
\end{itemize}
\subsection{Logistic Regression Classifier}
Logistic Regression is a discriminative classification model. Starting from the results obtained
from the Gaussian classifiers we consider the linear decision function obtained from the expression
of the posterior log-likelihood ratio:
\begin{center}
	\begin{math}
		l(x) = log\;\frac{P(C=h_1|x)}{P(C=h_0|x)}=log\;\frac{f_{X|C}(x|h_1)}{f_{X|C}(x|h_0)} + log\;\frac{\pi}{1-\pi} = w^Tx + b
	\end{math}
\end{center}
where $b$ takes into account all the prior information. Given $w$ and $b$ we can compute the
expression for the posterior class probability:
\begin{center}
	\begin{math}
		P(c=h_1|x,w,b)=\frac{e^{(w^Tx + b)}}{1+e^{(w^Tx+b)}}=\sigma(w^Tx+b)
	\end{math}
\end{center}
where $\sigma(x)=\frac{1}{1+e^{-x}}$ is the sigmoid function. Decision rules will be hyperplanes
orthogonal to $w$.
\subsubsection{Linear Logistic Regression}
We are going to look for the minimizer of the function:
\begin{center}
	\begin{math}
		J(w,b) = \frac{\lambda}{2}||w||^2 + \frac{1}{n}\sum_{i=1}^{n}log(1 + e^{-z_i(w^Tx_i+b)})
	\end{math}
\end{center}
where $\lambda$ is an hyperparameter that represents the regularization term (needed to make the 
problem solvable in case of linearly separable classes).
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/dcfplotLLR.png}
	\caption{minDCF for different values of $\lambda$ and different priors}
	\label{dcfLLR} 
\end{figure}
\begin{table}[ht!]
		\caption{Linear Logistic Regression - 3-fold cross validation}
		\centering
		\begin{tabular}{ |l|l|l|l| }
			\hline
			& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
			\hline
			\multirow{3}{*}{}
			 LLR \scriptsize{($\lambda=10^{-3}$)}& \textcolor{blue}{0.170} & 0.059 & 0.155\\
			 LLR \scriptsize{($\lambda=10^{-5}$)}& 0.132 & \textcolor{red}{0.047} & 0.127\\
			 LLR \scriptsize{($\lambda=10^{-6}$)}& 0.132 & \textcolor{red}{0.047} & \textcolor{orange}{0.126}\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=11)}} \\
			\hline
			\multirow{3}{*}{}
			 LLR \scriptsize{($\lambda=10^{-3}$)}& \textcolor{blue}{0.278} & 0.104 & 0.232\\
			 LLR \scriptsize{($\lambda=10^{-5}$)}& 0.269 & \textcolor{red}{0.098} & 0.221\\
			 LLR \scriptsize{($\lambda=10^{-6}$)}& \textcolor{orange}{0.268} & 0.098 & 0.222\\
			 \hline
			 \multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
			 \hline
			 \multirow{3}{*}{}
			  LLR \scriptsize{($\lambda=10^{-3}$)}& \textcolor{blue}{0.299} & \textcolor{red}{0.113} & 0.263\\
			  LLR \scriptsize{($\lambda=10^{-5}$)}& \textcolor{orange}{0.297} & 0.114 & 0.263\\
			  LLR \scriptsize{($\lambda=10^{-6}$)}& 0.297 & 0.114 & \textcolor{orange}{0.262}\\
			 \hline
		\end{tabular}
	\end{table}
\begin{table}[ht!]
		\caption{Best models analyzed up to now}
		\centering
		\begin{tabular}{ |l|l|l|l| }
			\hline
			& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
			\multicolumn{4}{ |c| }{\bf{Gaussian Models}} \\
			\hline
			\multirow{2}{*}{}
			 Tied Cov \scriptsize{(Z-Norm, no PCA)}& 0.122 & 0.046 & 0.127\\
			 Tied Cov \scriptsize{(Gau, PCA(m=10))}& 0.215 & 0.082 & 0.207\\
			\hline
		\end{tabular}
\end{table}
The choice of $\lambda$ appear to be critical for all the applications, in particular for the 
unbalanced ones. By observing figure \ref{dcfLLR} it is clear that for values of $\lambda$ greater
than $10^-3$ the $minDCF$ rapidly increases for all the considered applications. (WHY). 
Best performances are obtained for small values of $\lambda$. PCA with $m=10$ helps in obtaining better
results for the target application with $\lambda=10^{-3}$, however, better results are achieved for $\lambda=10^-5$
and without PCA.\\
\textit{Comparison}: the LLR model trained with $\lambda=10^{-5}$ and no-PCA achieves really similar results
with the ones achieved by the Tied-Cov Gaussian model (Z-Norm, no PCA) for $\tilde{\pi}=0.5$ and $\tilde{\pi}=0.9$
applications but a worse performance is obtained for $\tilde{\pi}=0.1$. The LLR model behaves better than the
Tied-Cov Gaussian model (Gau, PCA(m=10)) in all the three applications.\\
\underline{Selected LLR Model}: 
\begin{itemize}
	\item Z-normalized features, $\lambda=10^{-6}$, no PCA
\end{itemize}
\subsubsection{Quadratic Logistic Regression}
Now we are going to train a Quadratic LR model by performing features expansion.
For binary linear LR the separation surfaces are linear decision function as already discussed (and
we obtain the same form as for the Tied Gaussian classifier). By looking instead at the separation surface
obtained through the MVG gaussian classifier we have:
\begin{center}
	\begin{math}
		log\;\frac{P(C=h_1|x)}{P(C=h_0|x)} = x^TAx + b^Tx + c = s(x, A, b, c)
	\end{math}
\end{center}
This expression is quadratic in $x$ but linear in $A$ and $b$. 
We could rewrite it to obtain a decision function that is linear for the expanded features space
but quadratic in the original features space. Features expansion is defined as:
\begin{center}
	\begin{math}
		\Phi(x) = \begin{bmatrix}
					vec(xx^T)\\
					x
				  \end{bmatrix} \;, 
		w= 		 \begin{bmatrix}
					vec(A)\\
					b
				  \end{bmatrix}
	\end{math}
\end{center}
where $vec(X)$ is the operator that stacks the columns of $X$. In this way the posterior log-likelihood is expressed as:
\begin{center}
	\begin{math}
		s(x, w, c) = s^T\phi(x)+c
	\end{math}
\end{center}
We are now going to train the Linear Logistic Regression model using features vectors $\phi(x)$.\\
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/dcfplotQLR.png}
	\caption{minDCF for different values of $\lambda$ and different priors}
	\label{dcfQLR} 
\end{figure}
\begin{table}[ht!]
	\caption{Quadratic Logistic Regression - 3-fold cross validation}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
		\hline
		\multirow{3}{*}{}
		 QLR \scriptsize{($\lambda=10^{-3}$)}& \textcolor{blue}{0.187} & 0.063 & 0.149\\
		 QLR \scriptsize{($\lambda=10^{-5}$)}& 0.153 & \textcolor{red}{0.052} & 0.142\\
		 QLR \scriptsize{($\lambda=10^{-6}$)}& 0.150 & 0.053 & 0.141\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=11)}} \\
		\hline
		\multirow{3}{*}{}
		 QLR \scriptsize{($\lambda=10^{-3}$)}& \textcolor{orange}{0.267} & 0.098 & \textcolor{orange}{0.222}\\
		 QLR \scriptsize{($\lambda=10^{-5}$)}& 0.263 & 0.098 & 0.230\\
		 QLR \scriptsize{($\lambda=10^{-6}$)}& 0.305 & \textcolor{red}{0.096} & 0.230\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
		\hline
		\multirow{3}{*}{}
		 QLR \scriptsize{($\lambda=10^{-3}$)}& 0.299 & 0.111 & 0.241\\
		 QLR \scriptsize{($\lambda=10^{-5}$)}& 0.307 & \textcolor{red}{0.109} & 0.249\\
		 QLR \scriptsize{($\lambda=10^{-6}$)}& 0.305 & 0.109 & 0.248\\
		\hline
	\end{tabular}
\end{table}
\begin{table}[ht!]
	\caption{Best models analyzed up to now}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Gaussian Models}} \\
		\hline
		\multirow{2}{*}{}
		 Tied Cov \scriptsize{(Z-Norm, no PCA)}& 0.122 & 0.046 & 0.127\\
		 Tied Cov \scriptsize{(Gau, PCA(m=10))}& 0.212 & 0.082 & 0.207\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Logistic Regression Models}} \\
		\hline
		\multirow{1}{*}{}
		LLR &&&\\\scriptsize{(Z-Norm, $\lambda=10^{-6}$, no PCA)}& 0.132 & 0.047 & 0.126\\
		\hline
	\end{tabular}
\end{table}
By rapidly looking at the results obtianed we can say that quadratic version of the logistic regression
performs worse w.r.t the linear version (the one without features expansion). The choice of $\lambda$
is still critical and $\lambda <= 10^{-5}$ still remains the best choice. Regarding the target application
we are able to reach similar results comparing to the linear version while the unbalanced applications
are more penalized. When PCA is applied no improvements are obtained. However we can notice how the best
choice for $\lambda$ for unbalanced applications changes when PCA is applied (with $\lambda=10^{-3}$ we 
achieve better results for $\tilde{{\pi}}=0.1$ and $\tilde{{\pi}}=0.9$ applications w.r.t. $\lambda=10^{-5}$ while for target application $\lambda=10^{-5}$ is the best choice)
\\
\textit{Comparison}: With respect to Gaussian models comparable performances are achieved for $\lambda=10^{-6}$ and
no PCA even if the model performs slightly worse. The quadratic model performs also worse than the linear model and
we won't consider it in score calibration.
\\ 
\underline{Selected QLR Model}: 
\begin{itemize}
	\item Z-Normalized features, $\lambda=10^{-5}$, no PCA
\end{itemize}
\subsection{SVM Classifier}
\subsubsection{Linear SVM}
Support Vector Machines are linear classifiers that look for maximum margin separation hyperplanes
The primal formulation consists in minimizing:
\begin{center}
	\begin{math}
		J(w,b)=\frac{1}{2}||w^2|| + C\sum_{i=1}^{N}max(0, 1-z_i(w^Tx_i+b))
	\end{math}
\end{center}
with $n$ the number of trainig samples and $C$ is an hyperparameter.\\
We have also considered the dual formulation to solve the SVM problem that consists in maximizing:
\begin{center}
	\begin{math}
		J^D(\alpha)=-\frac{1}{2}\alpha^TH\alpha + \alpha^T\textbf{1} \; s.t.\; 0 \le \alpha_i \le C, \forall i\in \{1,...,n\}\;,\;\sum_{i=1}^{n}\alpha_iz_i=0
	\end{math}
\end{center}
where $\textbf{1}$ is a $n$-dimensional vector of ones and $H$ is the matrix
whose elements are $H_ij = z_iz_jx_i^Tx_j$.\\
The relation between the primal and the dual formulation(between the maximizer values $w^*$ and $\alpha^*$) is:
\begin{center}
	$w^*=\sum_{i=1}^{n}\alpha_i^*z_ix_i$
\end{center}
and the optimal bias $b$ can be computed considering a sample $x_i$ that lies on the margin: $z_i(w^{*T}x_i+b^*)=1$
To be able to computationally solve the problem
we need to modify the primal formulation as:
\begin{center}
	\begin{math}
		\hat{J}(\hat{w})=\frac{1}{2}||\hat{w}||^2 + C\sum_{i=1}^{N}max(0, 1-z_i(\hat{w}^T\hat{x}_i))
	\end{math}
\end{center}
where $\hat{x}_i = \begin{bmatrix} x_i\\ 1 \end{bmatrix}$ and $\hat{w} = \begin{bmatrix} w\\ b \end{bmatrix}$.\\
The scoring rule $\hat{w}^T\hat{x}_i = w^Tx + b$ has the same form of the original formulation but
we are also regularizing the norm of $\hat{w}: ||\hat{w}||^2 = ||w||^2 + b^2$ and we use a mapping
$\hat{x}_i = \begin{bmatrix} x_i\\ K \end{bmatrix}$ to mitigate the fact that by regularizing  the bias
term we could obtain sub-optimal results. The bigger is $K$ the lower the regularization effect of $b$
becomes weaker and the dual formulation becomes harder to solve. According to the modification done to
the primal formulation we also modify the dual formulation as:
\begin{center}
	\begin{math}
		J^D(\alpha)=-\frac{1}{2}\alpha^T\hat{H}\alpha + \alpha^T\textbf{1} \; s.t.\; 0 \le \alpha_i \le C, \forall i\in \{1,...,n\}
	\end{math}
\end{center}
where the equality constraint dissapeared (the one that L-BFGS was not able to incorporate) and the
matrix $\hat{H}$ can be computed as $\hat{H}_{i,j}=z_iz_j\hat{x}_i^T\hat{x}_j$
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/LSVM.png}
	\caption{Linear SVM (K=0, Z-Normalized features) - minDCF for different values of $C$ and different priors}
	\label{lsvm} 
\end{figure}
\begin{table}[ht!]
	\caption{Linear SVM - 3-fold cross validation}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
		\hline
		\multirow{2}{*}{}
		 Linear SVM \scriptsize{($C=0.1$)}& \textcolor{blue}{0.158} & 0.052 & 0.141\\
		 Linear SVM \scriptsize{($C=1$)}& 0.129 & \textcolor{red}{0.047} & 0.130\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=11)}} \\
		\hline
		\multirow{2}{*}{}
		Linear SVM \scriptsize{($C=0.1$)}& \textcolor{blue}{0.275} & 0.102 & 0.233\\
		Linear SVM \scriptsize{($C=1$)}& 0.269 & \textcolor{red}{0.100} & 0.226\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
		\hline
		\multirow{2}{*}{}
		Linear SVM \scriptsize{($C=0.1$)}& 0.295 & 0.114 & 0.259\\
		Linear SVM \scriptsize{($C=1$)}& \textcolor{blue}{0.301} & \textcolor{red}{0.113} & 0.267\\
		\hline
	\end{tabular}
\end{table}

\begin{table}[ht!]
	\caption{Best models analyzed up to now}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Gaussian Models}} \\
		\hline
		\multirow{2}{*}{}
		 Tied Cov \scriptsize{(Z-Norm, no PCA)}& 0.122 & 0.046 & 0.127\\
		 \hline
		 Tied Cov \scriptsize{(Gau, PCA(m=10))}& 0.212 & 0.082 & 0.207\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Logistic Regression Models}} \\
		\hline
		\multirow{2}{*}{}
		LLR &&&\\\scriptsize{(Z-Norm, $\lambda = 10^{-6}$, no PCA)}& 0.132 & 0.047 & 0.126\\
		\hline
		QLR &&&\\\scriptsize{(Z-Norm, $\lambda = 10^{-5}$, no PCA)}& 0.153 & 0.052 & 0.142\\
		\hline
	\end{tabular}
\end{table}
The only preprocessing step that is considered in Z-normalization. As we can notice from figure \ref{lsvm}
better results are achieved for smaller values of the hyperparameter $C$ so the choice of $C$ is crucial.
By considering models trained for $C=0.1$ and $C=1$ we can observ that best results are obtained by the 
target application when no PCA is applied. PCA with $m=10$ or $m=0$ obtained worse performance for
all the three applications.\\
\underline{Selected Linear SVM Model}: C=1 - Z-normalized features - no PCA
\subsubsection{Kernel SVM}
SVMs allow for non-linear classification through an implicit expansion of the features in a higher
dimensional space. In contrast with Quadratic Logistic Regression classifier we don't have to compute
an explicit expansion of the features space, it is sufficient to be able to compute the scalar product between
the expanded features $k(x_1, x_2) = \phi(x_1)^T\phi(x_2)$ and $k$ is the kernel function. We have to 
replace $\hat{H}$ with $\hat{H} = z_iz_jk(z_1,x_2)$. We are going to try two types of kernel: polynomial and rbf.
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/polysvm.png}
	\caption{Polynomial SVM (K=1, c=0, d=2, raw features) - minDCF for different values of C}
	\label{polysvm} 
\end{figure}
The model was trained with raw features since the Z-Normalization steps brought to very poor results
for almost all values of the hyperparameter C. By looking at figure \ref{polysvm} we can
realize that the choice of $C$ is crucial and best performance are obtained for $C \le10^{-4}$.
For this reason i have chosen $C=10^{-4}$ to show more detailed obtained results.
\begin{table}[ht!]
	\caption{Polynomial Kernel SVM - $C=10^{-4}$}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Raw features - no PCA}} \\
		\hline
		\multirow{2}{*}{}
		 Poly SVM \scriptsize{(c=0, d=2)}& 0.465 & 0.158 & 0.396\\
		 Poly SVM \scriptsize{(c=1, d=2)}& 0.187 & \textcolor{red}{0.060} & 0.164\\
		 Poly SVM \scriptsize{(c=1, d=3)}& 0.408 & 0.157 & \textcolor{blue}{0.563}\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Raw features - PCA(m=11)}} \\
		\hline
		\multirow{2}{*}{}
		Poly SVM \scriptsize{(c=0, d=2)}& \textcolor{blue}{0.553} & 0.199 & 0.487\\
		Poly SVM \scriptsize{(c=1, d=2)}& 0.210 & \textcolor{red}{0.062} & \textcolor{orange}{0.158}\\
		Poly SVM \scriptsize{(c=1, d=3)}& 0.452 & 0.183 & 0.537\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Raw features - PCA(m=10)}} \\
		\hline
		\multirow{2}{*}{}
		Poly SVM \scriptsize{(c=0, d=2)}& 0.746 & 0.258 & 0.576\\
		Poly SVM \scriptsize{(c=1, d=2)}& 0.465 & \textcolor{red}{0.158} & 0.396\\
		Poly SVM \scriptsize{(c=1, d=3)}& \textcolor{blue}{0.784} & 0.280 & \textcolor{blue}{0.815}\\
		\hline
	\end{tabular}
\end{table}
\begin{itemize}
	\item The Polynomial SVM is able to achieve better results with respect to some versions
		  of the Linear SVM. Best performances for the target application are achieved
		  by the model trained with $c=1$ and $d=2$.
	\item The use of PCA(m=11) helps in reducing the minDCF for the $\tilde{\pi}=0.9$ application for the
		  model trained with $c=1$ and $d=2$ but gives in general no real benefits
		  to the model.
\end{itemize}
The other kernel that was employed is the RBF kernel and these are the results achieved. In this case
it was used Z-normalization as pre-processing step since it provides better results. The hyperparameter $\gamma$ has to be tuned
so i trained the model for different values of $\gamma$ to analyze the performances and here are showed
the results for $\gamma = 0.1$ and $\gamma = 0.01$.  
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/rbfsvm_znorm_compl.png}
	\caption{RBF SVM ($K=1$) - minDCF for different values of C}
	\label{rbfsvm_znorm} 
\end{figure}
The value of the hyperparameter $C$ is be critical and we should choose a value
$C \ge 10^{-1}$. The table below shows the results obtained with $C=10$.
\FloatBarrier
\begin{table}[ht!]
	\caption{RBF Kernel SVM - C=10}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Z-Normalized features - no PCA}} \\
		\hline
		\multirow{3}{*}{}
		 RBF SVM \scriptsize{($\gamma=1$)}& \textcolor{blue}{0.291} & 0.095 & 0.281\\
		 RBF SVM \scriptsize{($\gamma=0.1$)}& 0.163 & 0.056 & 0.137\\
		 RBF SVM \scriptsize{($\gamma=0.01$)}& 0.153 & \textcolor{red}{0.049} & 0.133\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-Normalized features - PCA(m=11)}} \\
		\hline
		\multirow{3}{*}{}
		RBF SVM \scriptsize{($\gamma=1$)}& \textcolor{blue}{0.362} & 0.123 & 0.350\\
		RBF SVM \scriptsize{($\gamma=0.1$)}& 0.255 & 0.092 & 0.255\\
		RBF SVM \scriptsize{($\gamma=0.01$)}& 0.254 & \textcolor{red}{0.091} & 0.211\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-Normalized features - PCA(m=10)}} \\
		\hline
		\multirow{3}{*}{}
		RBF SVM \scriptsize{($\gamma=1$)}& \textcolor{blue}{0.386} & 0.136 & 0.379\\
		RBF SVM \scriptsize{($\gamma=0.1$)}& 0.271 & \textcolor{red}{0.106} & 0.273\\
		RBF SVM \scriptsize{($\gamma=0.01$)}& 0.291 & 0.107 & 0.239\\
		\hline
	\end{tabular}
\end{table}
\FloatBarrier
\begin{itemize}
	\item RBF kernel version of the SVM achieves results that are near to the ones
		  obtained with the linear version and behaves better w.r.t. the Polynomial
		  version
	\item The most promising value of $\gamma$ seems to be $\gamma=0.01$ because with this
		  value we are able to obtain the best minDCF for the target application (when no
		  PCA is applied)
	\item PCA is not able to provide any better results in terms of minDCF
\end{itemize}
\textit{Comparison}: The linear version of the SVM achieves better results with
respect to the two kernel versions...
\underline{Selected Kernel SVM Models}:
\begin{itemize}
	\item Polynomial SVM: Raw features, $K=1$, $C=10^{-4}$, $c=1$, $d=2$, PCA(m=11)
	\item RBF SVM: Z-Normalized features, $K=1$, $C=10$, $\gamma=0.01$, no PCA
\end{itemize}
\begin{table}[ht!]
	\caption{Best models analyzed up to now}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Gaussian Models}} \\
		\hline
		\multirow{2}{*}{}
		 Tied Cov \scriptsize{(Z-Norm, no PCA)}& 0.122 & 0.046 & 0.127\\
		 \hline
		 Tied Cov \scriptsize{(Gau, PCA(m=10))}& 0.212 & 0.082 & 0.207\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Logistic Regression Models}} \\
		\hline
		\multirow{2}{*}{}
		LLR &&&\\\scriptsize{(Z-Norm, $\lambda = 10^{-6}$, no PCA)}& 0.132 & 0.047 & 0.126\\
		\hline
		QLR &&&\\\scriptsize{(Z-Norm, $\lambda = 10^{-5}$, no PCA)}& 0.153 & 0.052 & 0.142\\
		\hline
		\multicolumn{4}{ |c| }{\bf{SVM Models}} \\
		\hline
	\end{tabular}
\end{table}
%--
\subsection{GMM Classifier}
The last model we take into account is a generative model. We are going to train a GMM
over the samples of each class. Given the fact that almost all the features are already well
distributed according to the Gaussian hypothesis i suppose that we won't need many gaussian 
components for the model. Moreover, Tied GMM model is supposed to outperform the other two
models as already seen in the Gaussian classifiers.
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/gmmhist.png}
	\caption{GMM - minDCF for different number of components}
	\label{gmmhist} 
\end{figure}
As showed in figure \ref{gmmhist} for a relative small number of components (8) good results are
achieved by all the three types of models. Even if diag assumption doesn't hold really well
especially when the number of components is low it performs better with higher number of 
components. The Tied model is the one with best results even if the Full model still performs really
well. Gaussianization never helps in achieving better results. \\By considering only models trained
with 8 compoents we are now going to show the results obtained when training with and
without PCA.
\begin{table}[ht!]
	\caption{GMM - 3-fold cross validation}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
		\hline
		\multirow{3}{*}{}
		 GMM Full (8 comp.)& 0.109 & 0.040 & 0.102\\
		 GMM Tied (8 comp.)& 0.099 & \textcolor{red}{0.031} & 0.075\\
		 GMM Diag (8 comp.)& 0.214 & 0.085 & \textcolor{blue}{0.215}\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
		\hline
		\multirow{3}{*}{}
		GMM Full (8 comp.)& 0.225 & 0.082 & 0.212\\
		GMM Tied (8 comp.)& 0.204 & \textcolor{red}{0.072} & 0.186\\
		GMM Diag (8 comp.)& \textcolor{blue}{0.310} & 0.108 & 0.260\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=9)}} \\
		\hline
		\multirow{3}{*}{}
		GMM Full (8 comp.)& 0.389 & 0.150 & 0.361\\
		GMM Tied (8 comp.)& 0.347 & \textcolor{red}{0.133} & 0.328\\
		GMM Diag (8 comp.)& \textcolor{blue}{0.422} & 0.170 & 0.424\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Gaussianized features - no PCA}} \\
		\hline
		\multirow{3}{*}{}
		GMM Full (8 comp.)& 0.221 & 0.076 & 0.210\\
		GMM Tied (8 comp.)& 0.158 & 0.059 & 0.151\\
		GMM Diag (8 comp.)& 0.419 & 0.162 & 0.397\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=10)}} \\
		\hline
		\multirow{3}{*}{}
		GMM Full (8 comp.)& 0.274 & 0.096 & 0.257\\
		GMM Tied (8 comp.)& 0.185 & \textcolor{red}{0.069} & 0.185\\
		GMM Diag (8 comp.)& \textcolor{orange}{0.406} & \textcolor{orange}{0.148} & \textcolor{orange}{0.356}\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=9)}} \\
		\hline
		\multirow{3}{*}{}
		GMM Full (8 comp.)& 0.262 & 0.101 & 0.290\\
		GMM Tied (8 comp.)& 0.229 & \textcolor{red}{0.081} & 0.204\\
		GMM Diag (8 comp.)& 0.398 & 0.152 & 0.369\\
		\hline
	\end{tabular}
\end{table}

\begin{table}[ht!]
	\caption{Best models analyzed up to now}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Gaussian Models}} \\
		\hline
		\multirow{2}{*}{}
		 Tied Cov \scriptsize{(Z-Norm, no PCA)}& 0.122 & 0.046 & 0.127\\
		 Tied Cov \scriptsize{(Gau, PCA(m=10))}& 0.212 & 0.082 & 0.207\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Logistic Regression Models}} \\
		\hline
		\multirow{2}{*}{}
		GMM Full (8 comp.)& 0.225 & 0.082 & 0.212\\
		GMM Tied (8 comp.)& 0.204 & 0.072 & 0.186\\
		\hline
		\multicolumn{4}{ |c| }{\bf{SVM Models}} \\
		\hline
		\multirow{2}{*}{}
		GMM Full (8 comp.)& 0.389 & 0.150 & 0.361\\
		GMM Tied (8 comp.)& 0.347 & 0.133 & 0.328\\
		\hline
	\end{tabular}
\end{table}
As already discussed the Tied model is the one that achieves best results (especially for our target
application). PCA with $m=10$ doesn't affect too much the results obtained while it is helpful
in obtaing lower values for $minDCF$ in case of GMM-Diag with gaussianized features. As we can notice
\textcolor{orange}{here} the results are worse with respect to the ones obtained without features
gaussainization and PCA but they are better w.r.t. to the GMM-Diag with gaussianized features and without
PCA. WHen features gaussianization is applied the GMM-Diag model works better with a lower number of componenets (a similar
behaviour was also noticed in the firstly Gaussian models)
\section{Score Calibration}
\subsection{Calibration Analysis On Selected Models}
We now select one candidate for each of the previous analyzed classification models (Gaussian, Logisti Regression,
SVM, GMM):
\begin{itemize}
	\item \underline{Gaussian model}: Tied-Cov (Z-Normalization, no PCA)
	\item \underline{Logistic Regression}: Linear LR (Z-Normalization, no PCA)
	\item \underline{SVM}: 
	\item \underline{GMM}:
\end{itemize}
% Bayes error plot
% Comment on Bayes error plot
\subsection{Calbrating Scores For Selected Models}
We are going to transform the scores so that the theoretical threshold $t = -log\;\frac{\tilde{\pi}}{1-\tilde{\pi}}$
provides close to optimal values over a wild range of effective priors $\tilde{\pi}$. What we want to find is a 
monotonic function $f$ that maps not-calibrated scores in calibrated scores.\\
We assume that the function $f$ has the form:
\begin{center}
	$f(s) = \alpha s + \beta$
\end{center}
and $f(s)$ can be interpreted as log-likelihood ratio for the two class hypotheses:
\begin{center}
	$f(s) = log\;\frac{f_{S|C}(s|\mathcal{H}_T)}{f_{S|C}(s|\mathcal{H}_F)} = \alpha s + \beta$
\end{center}
and the class posterior probabilty for prior $\tilde{\pi}$ corresponds to:
\begin{center}
	$log\;\frac{P(C=\mathcal{H}_T|s)}{P(C=\mathcal{H}_F|s)} = \alpha s + \beta + log\;\frac{\tilde{\pi}}{1-\tilde{\pi}}$
\end{center}
By interpreting scores $s$ as samples of a dataset (each sample has 1 feature) we can employ a prior weighted
logistic regression model to learn the model parameters over our calibration set (the dataset composed of the
scores for a certain model). If we let:
\begin{center}
	$\beta' = \beta + log\;\frac{\tilde{\pi}}{1-\tilde{\pi}}$
\end{center}
we have exactly a Logistic Regression model where $\alpha$ and $\beta'$ are the model parameters we have to learn.
We have still to specify a prior $\tilde{\pi}$ that will be the one of our target application even if we will notice
that the improvements in term of calibration will also involve the unbalanced applications. To obtain calibrated
scores we will have to compute:
\begin{center}
	$f(s) = \alpha s + \beta = \alpha s + \beta' - log\;\frac{\tilde{\pi}}{1-\tilde{\pi}}$
\end{center}

%------------------------------------------------

\subsection{Subsection}

Nam ante risus, tempor nec lacus ac, congue pretium dui. Donec a nisl est. Integer accumsan mauris eu ex venenatis mollis. Aliquam sit amet ipsum laoreet, mollis sem sit amet, pellentesque quam. Aenean auctor diam eget erat venenatis laoreet. In ipsum felis, tristique eu efficitur at, maximus ac urna. Aenean pulvinar eu lorem eget suscipit. Aliquam et lorem erat. Nam fringilla ante risus, eget convallis nunc pellentesque non. Donec ipsum nisl, consectetur in magna eu, hendrerit pulvinar orci. Mauris porta convallis neque, non viverra urna pulvinar ac. Cras non condimentum lectus. Aliquam odio leo, aliquet vitae tellus nec, imperdiet lacinia turpis. Nam ac lectus imperdiet, luctus nibh a, feugiat urna.

\begin{itemize}
	\item First item in a list 
	\item Second item in a list 
	\item Third item in a list
\end{itemize}

Nunc egestas quis leo sed efficitur. Donec placerat, dui vel bibendum bibendum, tortor ligula auctor elit, aliquet pulvinar leo ante nec tellus. Praesent at vulputate libero, sit amet elementum magna. Pellentesque sodales odio eu ex interdum molestie. Suspendisse lacinia, augue quis interdum posuere, dolor ipsum euismod turpis, sed viverra nibh velit eget dolor. Curabitur consectetur tempus lacus, sit amet luctus mauris interdum vel. Curabitur vehicula convallis felis, eget mattis justo rhoncus eget. Pellentesque et semper lectus.

\begin{description}
	\item[First] This is the first item
	\item[Last] This is the last item
\end{description}

Donec nec nibh sagittis, finibus mauris quis, laoreet augue. Maecenas aliquam sem nunc, vel semper urna hendrerit nec. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Maecenas pellentesque dolor lacus, sit amet pretium felis vestibulum finibus. Duis tincidunt sapien faucibus nisi vehicula tincidunt. Donec euismod suscipit ligula a tempor. Aenean a nulla sit amet magna ullamcorper condimentum. Fusce eu velit vitae libero varius condimentum at sed dui.

%------------------------------------------------

\subsection{Subsection}

In hac habitasse platea dictumst. Etiam ac tortor fermentum, ultrices libero gravida, blandit metus. Vivamus sed convallis felis. Cras vel tortor sollicitudin, vestibulum nisi at, pretium justo. Curabitur placerat elit nunc, sed luctus ipsum auctor a. Nulla feugiat quam venenatis nulla imperdiet vulputate non faucibus lorem. Curabitur mollis diam non leo ullamcorper lacinia.

Morbi iaculis posuere arcu, ut scelerisque sem. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Mauris placerat urna id enim aliquet, non consequat leo imperdiet. Phasellus at nibh ut tortor hendrerit accumsan. Phasellus sollicitudin luctus sapien, feugiat facilisis risus consectetur eleifend. In quis luctus turpis. Nulla sed tellus libero. Pellentesque metus tortor, convallis at tellus quis, accumsan faucibus nulla. Fusce auctor eleifend volutpat. Maecenas vel faucibus enim. Donec venenatis congue congue. Integer sit amet quam ac est aliquam aliquet. Ut commodo justo sit amet convallis scelerisque.

\begin{enumerate}
	\item First numbered item in a list
	\item Second numbered item in a list
	\item Third numbered item in a list
\end{enumerate}

Aliquam elementum nulla at arcu finibus aliquet. Praesent congue ultrices nisl pretium posuere. Nunc vel nulla hendrerit, ultrices justo ut, ultrices sapien. Duis ut arcu at nunc pellentesque consectetur. Vestibulum eget nisl porta, ultricies orci eget, efficitur tellus. Maecenas rhoncus purus vel mauris tincidunt, et euismod nibh viverra. Mauris ultrices tellus quis ante lobortis gravida. Duis vulputate viverra erat, eu sollicitudin dui. Proin a iaculis massa. Nam at turpis in sem malesuada rhoncus. Aenean tempor risus dui, et ultrices nulla rutrum ut. Nam commodo fermentum purus, eget mattis odio fringilla at. Etiam congue et ipsum sed feugiat. Morbi euismod ut purus et tempus. Etiam est ligula, aliquam eget porttitor ut, auctor in risus. Curabitur at urna id dui lobortis pellentesque.


%------------------------------------------------

\section{Section}

\begin{figure}[htpb!]
	\includegraphics[width=\linewidth]{bear.jpg} % Figure image
	\caption{A majestic grizzly bear} % Figure caption
	\label{bear} % Label for referencing with \ref{bear}
\end{figure}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography[title={Bibliography}] % Print the bibliography, section title in curly brackets

%----------------------------------------------------------------------------------------

\end{document}
