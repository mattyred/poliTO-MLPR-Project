%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Article
% LaTeX Template
% Version 2.0 (28/2/17)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[10pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)
\usepackage{placeins}
\usepackage{multirow}
\usepackage{amsmath}
\newcommand{\comment}[1]{}
\input{structure.tex} % Specifies the document structure and loads requires packages

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{MLPR Exam Project: \\Gender Detection} % The article title

\author{
	\authorstyle{Mattia Rosso [s294711]} % Author
	%\newline\newline % Space before institutions
	%\textsuperscript{1}\institution{Universidad Nacional Autónoma de México, Mexico City, Mexico}\\ % Institution 1
	%\textsuperscript{2}\institution{University of Texas at Austin, Texas, United States of America}\\ % Institution 2
	%\textsuperscript{3}\institution{\texttt{LaTeXTemplates.com}} % Institution 3
}

% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}

\date{\today} % Add a date here if you would like one to appear underneath the title block, use \today for the current date, leave empty for no date

%----------------------------------------------------------------------------------------

\begin{document}
\maketitle % Print the title
\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{This project is inteded to show a binary classification 
task on a datased made of 12 continuous observations coming from speking embeddings. 
A speaker embedding represents a smal-dimensional, fixed size representation of an utterance.
Features can be seen as points in the m-dimensional embedding space (and the embeddings
have already been computed). This is a task where classes are balanced both in training and
evaluation set}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Dataset analysis}
\subsection{Training and evaluation sets}
The training set contains:
\begin{itemize}
	\item Training Set: 3000 samples belonging to Male class (Label = 0) and
						3000 samples belonging to Female class (Label = 1).
	\item Evaluation Set: 2000 samples belonging to Male class (Label = 0) and
						  2000 samples belonging to Female class (Label = 1).
\end{itemize}

\subsection{Training set features analysis}
\subsection{Features Statistics}
All the features are contiguous and their main statics can be
showed through a boxplot in figure \ref{boxplot}. 

\subsection{Z-normalization}
A useful operation that can be applied in order to avoid to deal with numerical 
issues and to make data more uniform is to apply Z-normalization as a preprocessing step:\\
\begin{center}
	\begin{math}
		z_{i,j} = \frac{x_{i,j} - \mu_{j}}{\sigma_{j}} \forall x_{i} \in D 
	\end{math}
\end{center}
Where $z_{i,j}$ is the Z-normalized value correspongin to the feature $j$
of sample $i$ while $\mu_{j}$ and $\sigma_{j}$ are, respectively,
the mean and the variance computed over all values for feature $j$.
\comment{
\FloatBarrier
	\begin{table}
		\caption{Features Statistics}
		\centering
		\begin{tabular}{ |l|l|l|l|l| }
			\hline
			\multicolumn{5}{ |c| }{Statistics} \\
			\hline
			Feature & Min & Max & Mean & StdDev \\ \hline
			\multirow{2}{*}{0}
			 & 0 & 0 & 0 & 0 \\
			 & 0 & 0 & 0 & 0  \\ \hline
			\multirow{2}{*}{1}
			 & 0 & 0 & 0 & 0  \\
			 & 0 & 0 & 0 & 0  \\ \hline
			\multirow{2}{*}{2}
			& 0 & 0 & 0 & 0  \\
			& 0 & 0 & 0 & 0  \\ \hline
		\end{tabular}
	\end{table}
\FloatBarrier
}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/boxplot.png}
	\caption{Features boxplot}
	\label{boxplot} 
\end{figure}

\subsection{Features distribution}
By plotting historams for each feature, separated for male and female, it is possible to show
how much each feature follows a gaussian distribution in order to understend whether a pre processing
like gaussianization can be useful to go on with our classification task
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/hist_znorm.png}
	\caption{Z-normalized features distribution}
	\label{hist_znorm} 
\end{figure}

We can notice from figure \ref{hist_znorm} that almost all the features are already 
well-distributed except for features 3, 7, 9. We can apply an additional pre-processing step in
order to make all the features following a gaussian distribution.

\subsubsection{Gaussianization}
Gaussianization is a pre processing step that maps each fature to values whose empirical cumulative
distribution is well approximated by a Gaussian cumulative distribution function. For each
feature $x$ that we want to gaussianize we firstly compute the rank over the dataset:\\
\begin{center}
	\begin{math}
		r(x) = \frac{\sum_{i=1}^{N}\mathbb{I}[x_{i} < x] + 1}{N + 2}
	\end{math}
\end{center}
where $\mathbb{I}$ is the indicator function ($1$ when the condition inside $[ ]$ is true, $0$ 
otherwise). Actually, we are counting how many samples in the dataset $D$ have a greater value
with respect to the feature we are computing the rank on. \\
The next step is to compute the transformed feature as $y = \Phi^{-1}(r(x))$ where $\Phi$ is
the inverse of the cumulative distribution function. 

\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/hist_gau.png}
	\caption{Gaussianized features distribution}
	\label{hist_gau} 
\end{figure}

\subsection{Features correlation}
We can show how much features are correlated by using a heatmap plot showing a darker
color inside cells $[i, j]$ for which it exists an high correlation among feaure $i$ and 
feature $j$. We are going to use the Pearson correlation coefficient to compute the
correlation among feature $X$ and feature $Y$:
\begin{center}
	\begin{math}
		|\frac{Cov(X, Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}|
	\end{math}
\end{center}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/heatmap_znormgau.png}
	\caption{Z-normalized+Gaussianized features correlation: grey the whole dataset, orange F class, 
	blue M class}
	\label{heatmap} 
\end{figure}

Correlation is quite high among most of the features, we can understand that applying PCA
would be meaningful for values of $m$ not below $10$ or $9$ at most. For smaller values of $m$
we would lose important information coming from high-correlated features. 

\section{Dimensionality reduction}
Before proceeding with the classification task i would spend some words on the possible 
dimensioanlity reduction rechniques that can be applied: PCA, LDA.
\subsection{PCA}
As already anticipated, given the heatmap in figure \ref{heatmap} we can observe that PCA
with reasonable values of $m$ can be applied. PCA is a dimensionality reduction technique
that, given a centered dataset $X = {x_{1}, ..., x_{k}}$, it aims to find the subspace of 
$\mathbb{R}^{n}$ that allows to preserve most of the information (the directions with the
highest variance).\\Starting from the sample covariance matrix
\begin{center}
	\begin{math}
		C = \frac{1}{K} \sum_{i}^{}(x_{i} - \bar{x})(x_{i}-\bar{x})^T
	\end{math}
\end{center}
we compute the eigen-decomposition of $C = U\Sigma U^T$ and project the data in the subspace
spanned by the $m$ columns of $U$ corresponding to the $m$ highest eigenvalues:
\begin{center}
	\begin{math}
		y_{i} = P^T(x_{i}-\bar{x})
	\end{math}
\end{center}
In order to select the optimal $m$ we can use a cross-validation approach by inspecting how much
of the total variance of data we are able to retain by using that value of $m$. We exploit the fact
that each eigenvalue correspond to the variance along the corresponding axis and the
eigenvalues are the elements of the diagonal of the matrix $\Sigma$. We selecte $m$ as:
\begin{center}
	\begin{math}
		\min_m\;s.t \frac{\sum_{i}^{m}\sigma_{i}}{\sum_{i}^{n}\sigma_{i}} >= t,\;t \ge 95\%
	\end{math}
\end{center}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/pca.png}
	\caption{3-fold cross validation for PCA impact evaluation}
	\label{pca} 
\end{figure}
We can clearly understand from figure \ref{pca} that values of $m < 9$ would rapidly
decrease the amout of retained variance. We will see better later on how this would 
badly impact the performance of the classifiers

\subsection{LDA}
PCA technique is unsupervised so we have no guarantee of obtainig discriminant directions.
Despite the fact that LDA allows to find at most $C - 1$, where $C$ is the number of classes,
directions, so it makes no sense to apply it as a dimensionality reduction technique, it can
be used as a linear classifier and we can understand it by its definition; LDA mazimizes the
between-class variability over the within-class variability ratio for the transformed samples:
\begin{center}
	\begin{math}
		\mathcal{L}(w) = \frac{s_{B}}{s_{W}} = \max_w \frac{w^TS_{B}w}{w^TS_{W}w}
	\end{math}
\end{center}
It can be proved that optimal solution correspond to the eigenvector of $S_{W}^-1S_{B}$ 
corresponding to the largest eigenvalue. Once that we have estimated $w$ we can project
the test samples over $w$ and assign the class label looking at the score obtained:\\
\begin{center}
	\begin{math}
		f(x)=\left\{
		\begin{array}{ll}
			1, & \mbox{if $x<0$}.\\
			0, & \mbox{otherwise}.
		\end{array}
		\right.
	\end{math}
\end{center}
It will be showed later the result of the classifier
\section{Classification models analysis}
\subsection{Premises}
In the next paragraphs we are going to compare different classification models. 
We will employ a k-fold cross validation technique (with $k=3$) for model evaluation and the
best models will be chosen to train the entire training set for performing a final comparison.
We will consider three types of applications:
\begin{center}
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.1, 1, 1)$\\
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.5, 1, 1)$\\
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.9, 1, 1)$\\
\end{center}
and the target application will be 
\begin{center}
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.5, 1, 1)$
\end{center}
We are interested
in selecting the most promising approach and we will infact perform measures in term of minimum
detection cost:
\begin{center}
	\begin{math}
		DCF = \frac{DCF_{u}(\pi_{T}, C_{fn}, C_{fp})}{min(\pi_{T},C_{fn}, (1-\pi_{T}C_{fp}))} = 
			\frac{\pi_{T}C_{fn}P_{fn} + (1-\pi_{T})C_{fp}P_{fp}}{min(\pi_{T},C_{fn}, (1-\pi_{T}C_{fp}))}
	\end{math}
\end{center}
and for $minDCF$ computation we will look for:
\begin{center}
	\begin{math}
		t' = -log(\frac{\tilde{\pi}}{1-\tilde{\pi}})
	\end{math}
\end{center}
that allows us to obtain the lowest possible $DCF$ (as if knew in advance this optimal threshold)
\subsection{Gaussian models}
The first class of models we are going to analyze are the generative gaussian models. Model assumption are that,
given the dataset $X$ we assume that the sample $x_{t}$ is a realization of the R.V. $X_{t}$. A simple
model consists in assuming that our data, given the class, can be described by a Gaussian distribution:
\begin{center}
	\begin{math}
		(X_{t}|C_{t}=c) \sim (X|C=c) \sim \mathcal{N} (x_{t}|\mu_{c},\Sigma_{c})
	\end{math}
\end{center}
We will assign a probabilistic score to each sample in term of the class-posterior 
log-likelihood ratio:
\begin{center}
	\begin{math}
		log\;r(x_{t}) = log\;\frac{P(C = h_{1}|x_{t})}{P(C = h_{0}|x_{t})}
	\end{math}
\end{center}
We can expand this expression by writing:
\begin{center}
	\begin{math}
		log\;r(x_{t}) = log\;\frac{f_{X|C}(x_{t}|h_{1})}{f_{X|C}(x_{t}|h_{0})} + log\;\frac{\pi}{1-\pi}
	\end{math}
\end{center}
While the training phase consists in estimating the model parameters the scoring phase
consists in computing the log-likelihood ratio (first term of the equation) for each sample. 
It will be then compared with  a threshold specific for each application for computing 
the $minDCF$. What differentiate the different Gaussian models is the way how we estimate the
model parameters.
\subsection{MVG Gaussian Classifier}
The ML solution to the previous desribed problem is given by the empirical mean and covariance
matrix for each class:
\begin{center}
	$\mu_{c}^* = \frac{1}{N_c}\sum_{i=1}^{N}x_{c,i}$ \\
	$\Sigma_c^* = \frac{1}{N_c}\sum_{i=1}^{N}(x_{c,i}-\mu_c^*)(x_{c,i}-\mu_c^*)$
\end{center}
We will then compute the log densities for each sample by using the estimated model parameters
\subsection{Naive Bayes Classifier}
The Naive Bayes assumption simplifies the MVG full covariance model stating that if we knew
that for each class the componenets are approximately independent we can assume that the 
distribution $X|C$ can be factorized over its components. The ML solution to this problem is:
\begin{center}
	$\mu_{c,[j]}^* = \frac{1}{N_c}\sum_{i|c_i=c}^{}x_{i,[j]}$ \\
	$\sigma_{c,[j]}^2 = \frac{1}{N_c}\sum_{i|c_i=c}^{}(x_{i,[j]}-\mu_{c,[j]})^2$
\end{center}
The density of a sample $x$ can be expressed as $\mathcal{N}(x|\mu_c,\Sigma_c)$ where
$\mu_c$ is an array where each element $\mu_{c,[j]}$ is the the mean for each class
for each component while $\Sigma_c$ is a diagonal covariance matrix. The Naive Bayes classifier
corresponds to the MVG full covariance classifier with a diagonal covariance matrix
\subsection{Tied Gaussian Classifier}
This model assumes that the covariance matrices of the different class are tied (we consider
only one covariance matrix common to all classes). We are assuming that:
\begin{center}
	$f_{X|C}(x|c) = \mathcal{N} (x|\mu_c, \Sigma)$
\end{center}
so each class has its own mean but the covariance matrix is the same for all the classes. The ML
solution to this problem is:
\begin{center}
	$\mu_{c}^* = \frac{1}{N_c}\sum_{i=1}^{N}x_{c,i}$ \\
	$\Sigma^* = \frac{1}{N}\sum_c{}^{}\sum_{i|c_i=c}^{}(x_{i}-\mu_c^*)(x_{i}-\mu_c^*)$
\end{center}
This model is strongly related to LDA (used as a linear classfication model). By considering 
the binary log-likelihood ratio of the tied model we obtain a linear decision function:
\begin{center}
	$llr(x) = log\;\frac{f_{X|C}(x|h_1)}{f_{X|C}(x|h_0)} = x^Tb + c$ 
\end{center}
where $b$ and $c$ are functions of class means and (tied) covariance matrix. On the other hand,
projecting over the LDA subspace is, up to a scaling factor $k$, given by:
\begin{center}
	$w^Tx = k \cdot x^T\varLambda (\mu_1-\mu_0)$ 
\end{center}
where $\varLambda (\mu_1-\mu_0) = b$. The LDA assumption that all the classes have the same
within class covariance is related to the assumption done for the tied model.\
\subsection{Gaussian Models Comparison}
\FloatBarrier
	\begin{table}
		\caption{MVG}
		\centering
		\begin{tabular}{ |l|l|l|l|l| }
			\hline
			& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ &\\ \hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.128 & 0.048 & 0.125&\\
			 Tied Cov & 0.122 & \textcolor{red}{0.046} & 0.127&\\
			 Naive Bayes & 0.822 & \textcolor{blue}{0.567} & 0.856&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.303 & 0.115 & 0.267&\\
			 Tied Cov & 0.293 & 0.112 & 0.264&\\
			 Naive Bayes & \textcolor{red}{0.306} & 0.121 & 0.283&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=9)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.398 & 0.153 & 0.369&\\
			 Tied Cov & 0.392 & 0.151 & 0.367&\\
			 Naive Bayes & 0.416 & 0.159 & 0.362&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - no PCA}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.218 & 0.078 & 0.191&\\
			 Tied Cov & 0.208 & 0.078 & 0.189&\\
			 Naive Bayes & 0.813 & 0.586 & 0.847 &\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=10)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.223 & 0.084 & 0.211&\\
			 Tied Cov & 0.212 & 0.082 & 0.207&\\
			 Naive Bayes & 0.279 & 0.103 & 0.254&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=9)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.270 & 0.105 & 0.247&\\
			 Tied Cov & 0.265 & 0.103 & 0.244&\\
			 Naive Bayes & 0.305 & 0.119 & 0.282&\\
			\hline
		\end{tabular}
	\end{table}
\FloatBarrier

A graphical version of the table can be helpful in analyzing results:
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/gaumodels.png}
	\caption{Top: Z-Normalized feautres, Bottom: Gaussianized features}
	\label{gauplots} 
\end{figure}

Some observations:
\begin{itemize}
	\item We can notice that Full-Cov model and Tied-Cov model achieve very good and similar results with
		  slighlty better performances for the tied model.
	\item 
	\item The models trained with gaussianization
		  pre processing perform overall better when PCA is applied (both for m=10 and m=9) while, in case of no PCA, the 
		  gaussianization step achieves slightly worse results
\end{itemize}
%------------------------------------------------

\subsection{Subsection}

Nam ante risus, tempor nec lacus ac, congue pretium dui. Donec a nisl est. Integer accumsan mauris eu ex venenatis mollis. Aliquam sit amet ipsum laoreet, mollis sem sit amet, pellentesque quam. Aenean auctor diam eget erat venenatis laoreet. In ipsum felis, tristique eu efficitur at, maximus ac urna. Aenean pulvinar eu lorem eget suscipit. Aliquam et lorem erat. Nam fringilla ante risus, eget convallis nunc pellentesque non. Donec ipsum nisl, consectetur in magna eu, hendrerit pulvinar orci. Mauris porta convallis neque, non viverra urna pulvinar ac. Cras non condimentum lectus. Aliquam odio leo, aliquet vitae tellus nec, imperdiet lacinia turpis. Nam ac lectus imperdiet, luctus nibh a, feugiat urna.

\begin{itemize}
	\item First item in a list 
	\item Second item in a list 
	\item Third item in a list
\end{itemize}

Nunc egestas quis leo sed efficitur. Donec placerat, dui vel bibendum bibendum, tortor ligula auctor elit, aliquet pulvinar leo ante nec tellus. Praesent at vulputate libero, sit amet elementum magna. Pellentesque sodales odio eu ex interdum molestie. Suspendisse lacinia, augue quis interdum posuere, dolor ipsum euismod turpis, sed viverra nibh velit eget dolor. Curabitur consectetur tempus lacus, sit amet luctus mauris interdum vel. Curabitur vehicula convallis felis, eget mattis justo rhoncus eget. Pellentesque et semper lectus.

\begin{description}
	\item[First] This is the first item
	\item[Last] This is the last item
\end{description}

Donec nec nibh sagittis, finibus mauris quis, laoreet augue. Maecenas aliquam sem nunc, vel semper urna hendrerit nec. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Maecenas pellentesque dolor lacus, sit amet pretium felis vestibulum finibus. Duis tincidunt sapien faucibus nisi vehicula tincidunt. Donec euismod suscipit ligula a tempor. Aenean a nulla sit amet magna ullamcorper condimentum. Fusce eu velit vitae libero varius condimentum at sed dui.

%------------------------------------------------

\subsection{Subsection}

In hac habitasse platea dictumst. Etiam ac tortor fermentum, ultrices libero gravida, blandit metus. Vivamus sed convallis felis. Cras vel tortor sollicitudin, vestibulum nisi at, pretium justo. Curabitur placerat elit nunc, sed luctus ipsum auctor a. Nulla feugiat quam venenatis nulla imperdiet vulputate non faucibus lorem. Curabitur mollis diam non leo ullamcorper lacinia.

Morbi iaculis posuere arcu, ut scelerisque sem. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Mauris placerat urna id enim aliquet, non consequat leo imperdiet. Phasellus at nibh ut tortor hendrerit accumsan. Phasellus sollicitudin luctus sapien, feugiat facilisis risus consectetur eleifend. In quis luctus turpis. Nulla sed tellus libero. Pellentesque metus tortor, convallis at tellus quis, accumsan faucibus nulla. Fusce auctor eleifend volutpat. Maecenas vel faucibus enim. Donec venenatis congue congue. Integer sit amet quam ac est aliquam aliquet. Ut commodo justo sit amet convallis scelerisque.

\begin{enumerate}
	\item First numbered item in a list
	\item Second numbered item in a list
	\item Third numbered item in a list
\end{enumerate}

Aliquam elementum nulla at arcu finibus aliquet. Praesent congue ultrices nisl pretium posuere. Nunc vel nulla hendrerit, ultrices justo ut, ultrices sapien. Duis ut arcu at nunc pellentesque consectetur. Vestibulum eget nisl porta, ultricies orci eget, efficitur tellus. Maecenas rhoncus purus vel mauris tincidunt, et euismod nibh viverra. Mauris ultrices tellus quis ante lobortis gravida. Duis vulputate viverra erat, eu sollicitudin dui. Proin a iaculis massa. Nam at turpis in sem malesuada rhoncus. Aenean tempor risus dui, et ultrices nulla rutrum ut. Nam commodo fermentum purus, eget mattis odio fringilla at. Etiam congue et ipsum sed feugiat. Morbi euismod ut purus et tempus. Etiam est ligula, aliquam eget porttitor ut, auctor in risus. Curabitur at urna id dui lobortis pellentesque.


%------------------------------------------------

\section{Section}

\begin{figure}[htpb!]
	\includegraphics[width=\linewidth]{bear.jpg} % Figure image
	\caption{A majestic grizzly bear} % Figure caption
	\label{bear} % Label for referencing with \ref{bear}
\end{figure}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography[title={Bibliography}] % Print the bibliography, section title in curly brackets

%----------------------------------------------------------------------------------------

\end{document}
