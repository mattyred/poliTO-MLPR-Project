%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Article
% LaTeX Template
% Version 2.0 (28/2/17)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[10pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)
\usepackage{placeins}
\usepackage{multirow}
\usepackage{amsmath}
\newcommand{\comment}[1]{}
\input{structure.tex} % Specifies the document structure and loads requires packages

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{MLPR Exam Project: \\Gender Detection} % The article title

\author{
	\authorstyle{Mattia Rosso [s294711]} % Author
	%\newline\newline % Space before institutions
	%\textsuperscript{1}\institution{Universidad Nacional Autónoma de México, Mexico City, Mexico}\\ % Institution 1
	%\textsuperscript{2}\institution{University of Texas at Austin, Texas, United States of America}\\ % Institution 2
	%\textsuperscript{3}\institution{\texttt{LaTeXTemplates.com}} % Institution 3
}

% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}

\date{\today} % Add a date here if you would like one to appear underneath the title block, use \today for the current date, leave empty for no date

%----------------------------------------------------------------------------------------

\begin{document}
\maketitle % Print the title
\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{This project is inteded to show a binary classification 
task on a datased made of 12 continuous observations coming from speking embeddings. 
A speaker embedding represents a smal-dimensional, fixed size representation of an utterance.
Features can be seen as points in the m-dimensional embedding space (and the embeddings
have already been computed). This is a task where classes are balanced both in training and
evaluation set}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Dataset analysis}
\subsection{Training and evaluation sets}
The training set contains:
\begin{itemize}
	\item Training Set: 3000 samples belonging to Male class (Label = 0) and
						3000 samples belonging to Female class (Label = 1).
	\item Evaluation Set: 2000 samples belonging to Male class (Label = 0) and
						  2000 samples belonging to Female class (Label = 1).
\end{itemize}

\subsection{Training set features analysis}
\subsection{Features Statistics}
All the features are contiguous and their main statics can be
showed through a boxplot in figure \ref{boxplot}. 

\subsection{Z-normalization}
A useful operation that can be applied in order to avoid to deal with numerical 
issues and to make data more uniform is to apply Z-normalization as a preprocessing step:\\
\begin{center}
	\begin{math}
		z_{i,j} = \frac{x_{i,j} - \mu_{j}}{\sigma_{j}} \forall x_{i} \in D 
	\end{math}
\end{center}
Where $z_{i,j}$ is the Z-normalized value correspongin to the feature $j$
of sample $i$ while $\mu_{j}$ and $\sigma_{j}$ are, respectively,
the mean and the variance computed over all values for feature $j$.
\comment{
\FloatBarrier
	\begin{table}
		\caption{Features Statistics}
		\centering
		\begin{tabular}{ |l|l|l|l|l| }
			\hline
			\multicolumn{5}{ |c| }{Statistics} \\
			\hline
			Feature & Min & Max & Mean & StdDev \\ \hline
			\multirow{2}{*}{0}
			 & 0 & 0 & 0 & 0 \\
			 & 0 & 0 & 0 & 0  \\ \hline
			\multirow{2}{*}{1}
			 & 0 & 0 & 0 & 0  \\
			 & 0 & 0 & 0 & 0  \\ \hline
			\multirow{2}{*}{2}
			& 0 & 0 & 0 & 0  \\
			& 0 & 0 & 0 & 0  \\ \hline
		\end{tabular}
	\end{table}
\FloatBarrier
}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/boxplot.png}
	\caption{Features boxplot}
	\label{boxplot} 
\end{figure}

\subsection{Features distribution}
By plotting historams for each feature, separated for male and female, it is possible to show
how much each feature follows a gaussian distribution in order to understend whether a pre processing
like gaussianization can be useful to go on with our classification task
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/hist_znorm.png}
	\caption{Z-normalized features distribution}
	\label{hist_znorm} 
\end{figure}

We can notice from figure \ref{hist_znorm} that almost all the features are already 
well-distributed except for features 3, 7, 9. We can apply an additional pre-processing step in
order to make all the features following a gaussian distribution.

\subsubsection{Gaussianization}
Gaussianization is a pre processing step that maps each fature to values whose empirical cumulative
distribution is well approximated by a Gaussian cumulative distribution function. For each
feature $x$ that we want to gaussianize we firstly compute the rank over the dataset:\\
\begin{center}
	\begin{math}
		r(x) = \frac{\sum_{i=1}^{N}\mathbb{I}[x_{i} < x] + 1}{N + 2}
	\end{math}
\end{center}
where $\mathbb{I}$ is the indicator function ($1$ when the condition inside $[ ]$ is true, $0$ 
otherwise). Actually, we are counting how many samples in the dataset $D$ have a greater value
with respect to the feature we are computing the rank on. \\
The next step is to compute the transformed feature as $y = \Phi^{-1}(r(x))$ where $\Phi$ is
the inverse of the cumulative distribution function. 

\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/hist_gau.png}
	\caption{Gaussianized features distribution}
	\label{hist_gau} 
\end{figure}

\subsection{Features correlation}
We can show how much features are correlated by using a heatmap plot showing a darker
color inside cells $[i, j]$ for which it exists an high correlation among feaure $i$ and 
feature $j$. We are going to use the Pearson correlation coefficient to compute the
correlation among feature $X$ and feature $Y$:
\begin{center}
	\begin{math}
		|\frac{Cov(X, Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}|
	\end{math}
\end{center}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/heatmap_znormgau.png}
	\caption{Z-normalized+Gaussianized features correlation: grey the whole dataset, orange F class, 
	blue M class}
	\label{heatmap} 
\end{figure}

Correlation is quite high among most of the features, we can understand that applying PCA
would be meaningful for values of $m$ not below $10$ or $9$ at most. For smaller values of $m$
we would lose important information coming from high-correlated features. 

\section{Dimensionality reduction}
Before proceeding with the classification task i would spend some words on the possible 
dimensioanlity reduction rechniques that can be applied: PCA, LDA.
\subsection{PCA}
As already anticipated, given the heatmap in figure \ref{heatmap} we can observe that PCA
with reasonable values of $m$ can be applied. PCA is a dimensionality reduction technique
that, given a centered dataset $X = {x_{1}, ..., x_{k}}$, it aims to find the subspace of 
$\mathbb{R}^{n}$ that allows to preserve most of the information (the directions with the
highest variance).\\Starting from the sample covariance matrix
\begin{center}
	\begin{math}
		C = \frac{1}{K} \sum_{i}^{}(x_{i} - \bar{x})(x_{i}-\bar{x})^T
	\end{math}
\end{center}
we compute the eigen-decomposition of $C = U\Sigma U^T$ and project the data in the subspace
spanned by the $m$ columns of $U$ corresponding to the $m$ highest eigenvalues:
\begin{center}
	\begin{math}
		y_{i} = P^T(x_{i}-\bar{x})
	\end{math}
\end{center}
In order to select the optimal $m$ we can use a cross-validation approach by inspecting how much
of the total variance of data we are able to retain by using that value of $m$. We exploit the fact
that each eigenvalue correspond to the variance along the corresponding axis and the
eigenvalues are the elements of the diagonal of the matrix $\Sigma$. We selecte $m$ as:
\begin{center}
	\begin{math}
		\min_m\;s.t \frac{\sum_{i}^{m}\sigma_{i}}{\sum_{i}^{n}\sigma_{i}} >= t,\;t \ge 95\%
	\end{math}
\end{center}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/pca.png}
	\caption{3-fold cross validation for PCA impact evaluation}
	\label{pca} 
\end{figure}
We can clearly understand from figure \ref{pca} that values of $m < 9$ would rapidly
decrease the amout of retained variance. We will see better later on how this would 
badly impact the performance of the classifiers

\subsection{LDA}
PCA technique is unsupervised so we have no guarantee of obtainig discriminant directions.
Despite the fact that LDA allows to find at most $C - 1$, where $C$ is the number of classes,
directions, so it makes no sense to apply it as a dimensionality reduction technique, it can
be used as a linear classifier and we can understand it by its definition; LDA mazimizes the
between-class variability over the within-class variability ratio for the transformed samples:
\begin{center}
	\begin{math}
		\mathcal{L}(w) = \frac{s_{B}}{s_{W}} = \max_w \frac{w^TS_{B}w}{w^TS_{W}w}
	\end{math}
\end{center}
It can be proved that optimal solution correspond to the eigenvector of $S_{W}^-1S_{B}$ 
corresponding to the largest eigenvalue. Once that we have estimated $w$ we can project
the test samples over $w$ and assign the class label looking at the score obtained:\\
\begin{center}
	\begin{math}
		f(x)=\left\{
		\begin{array}{ll}
			1, & \mbox{if $x<0$}.\\
			0, & \mbox{otherwise}.
		\end{array}
		\right.
	\end{math}
\end{center}
It will be showed later the result of the classifier
\section{Classification models analysis}
\subsection{Premises}
In the next paragraphs we are going to compare different classification models. 
We will employ a k-fold cross validation technique (with $k=3$) for model evaluation and the
best models will be chosen to train the entire training set for performing a final comparison.
We will consider three types of applications:
\begin{center}
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.1, 1, 1)$\\
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.5, 1, 1)$\\
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.9, 1, 1)$\\
\end{center}
and the target application will be 
\begin{center}
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.5, 1, 1)$
\end{center}
We are interested
in selecting the most promising approach and we will infact perform measures in term of minimum
detection cost:
\begin{center}
	\begin{math}
		DCF = \frac{DCF_{u}(\pi_{T}, C_{fn}, C_{fp})}{min(\pi_{T},C_{fn}, (1-\pi_{T}C_{fp}))} = 
			\frac{\pi_{T}C_{fn}P_{fn} + (1-\pi_{T})C_{fp}P_{fp}}{min(\pi_{T},C_{fn}, (1-\pi_{T}C_{fp}))}
	\end{math}
\end{center}
and for $minDCF$ computation we will look for:
\begin{center}
	\begin{math}
		t' = -log(\frac{\tilde{\pi}}{1-\tilde{\pi}})
	\end{math}
\end{center}
that allows us to obtain the lowest possible $DCF$ (as if knew in advance this optimal threshold)
\subsection{Gaussian models}
The first class of models we are going to analyze are the generative gaussian models. Model assumption are that,
given the dataset $X$ we assume that the sample $x_{t}$ is a realization of the R.V. $X_{t}$. A simple
model consists in assuming that our data, given the class, can be described by a Gaussian distribution:
\begin{center}
	\begin{math}
		(X_{t}|C_{t}=c) \sim (X|C=c) \sim \mathcal{N} (x_{t}|\mu_{c},\Sigma_{c})
	\end{math}
\end{center}
We will assign a probabilistic score to each sample in term of the class-posterior 
log-likelihood ratio:
\begin{center}
	\begin{math}
		log\;r(x_{t}) = log\;\frac{P(C = h_{1}|x_{t})}{P(C = h_{0}|x_{t})}
	\end{math}
\end{center}
We can expand this expression by writing:
\begin{center}
	\begin{math}
		log\;r(x_{t}) = log\;\frac{f_{X|C}(x_{t}|h_{1})}{f_{X|C}(x_{t}|h_{0})} + log\;\frac{\pi}{1-\pi}
	\end{math}
\end{center}
While the training phase consists in estimating the model parameters the scoring phase
consists in computing the log-likelihood ratio (first term of the equation) for each sample. 
It will be then compared with  a threshold specific for each application for computing 
the $minDCF$. What differentiate the different Gaussian models is the way how we estimate the
model parameters.
\subsubsection{MVG Gaussian Classifier}
The ML solution to the previous desribed problem is given by the empirical mean and covariance
matrix for each class:
\begin{center}
	$\mu_{c}^* = \frac{1}{N_c}\sum_{i=1}^{N}x_{c,i}$ \\
	$\Sigma_c^* = \frac{1}{N_c}\sum_{i=1}^{N}(x_{c,i}-\mu_c^*)(x_{c,i}-\mu_c^*)$
\end{center}
We will then compute the log densities for each sample by using the estimated model parameters
\subsubsection{Naive Bayes Classifier}
The Naive Bayes assumption simplifies the MVG full covariance model stating that if we knew
that for each class the componenets are approximately independent we can assume that the 
distribution $X|C$ can be factorized over its components. The ML solution to this problem is:
\begin{center}
	$\mu_{c,[j]}^* = \frac{1}{N_c}\sum_{i|c_i=c}^{}x_{i,[j]}$ \\
	$\sigma_{c,[j]}^2 = \frac{1}{N_c}\sum_{i|c_i=c}^{}(x_{i,[j]}-\mu_{c,[j]})^2$
\end{center}
The density of a sample $x$ can be expressed as $\mathcal{N}(x|\mu_c,\Sigma_c)$ where
$\mu_c$ is an array where each element $\mu_{c,[j]}$ is the the mean for each class
for each component while $\Sigma_c$ is a diagonal covariance matrix. The Naive Bayes classifier
corresponds to the MVG full covariance classifier with a diagonal covariance matrix
\subsubsection{Tied Gaussian Classifier}
This model assumes that the covariance matrices of the different class are tied (we consider
only one covariance matrix common to all classes). We are assuming that:
\begin{center}
	$f_{X|C}(x|c) = \mathcal{N} (x|\mu_c, \Sigma)$
\end{center}
so each class has its own mean but the covariance matrix is the same for all the classes. The ML
solution to this problem is:
\begin{center}
	$\mu_{c}^* = \frac{1}{N_c}\sum_{i=1}^{N}x_{c,i}$ \\
	$\Sigma^* = \frac{1}{N}\sum_c{}^{}\sum_{i|c_i=c}^{}(x_{i}-\mu_c^*)(x_{i}-\mu_c^*)$
\end{center}
This model is strongly related to LDA (used as a linear classfication model). By considering 
the binary log-likelihood ratio of the tied model we obtain a linear decision function:
\begin{center}
	$llr(x) = log\;\frac{f_{X|C}(x|h_1)}{f_{X|C}(x|h_0)} = x^Tb + c$ 
\end{center}
where $b$ and $c$ are functions of class means and (tied) covariance matrix. On the other hand,
projecting over the LDA subspace is, up to a scaling factor $k$, given by:
\begin{center}
	$w^Tx = k \cdot x^T\varLambda (\mu_1-\mu_0)$ 
\end{center}
where $\varLambda (\mu_1-\mu_0) = b$. The LDA assumption that all the classes have the same
within class covariance is related to the assumption done for the tied model.\
\subsubsection{Gaussian Models Comparison}
\FloatBarrier
	\begin{table}
		\caption{MVG}
		\centering
		\begin{tabular}{ |l|l|l|l|l| }
			\hline
			& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ &\\ \hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.128 & 0.048 & 0.125&\\
			 Tied Cov & 0.122 & \textcolor{red}{0.046} & 0.127&\\
			 Naive Bayes & 0.822 & \textcolor{blue}{0.567} & 0.856&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.303 & 0.115 & 0.267&\\
			 Tied Cov & 0.293 & 0.112 & 0.264&\\
			 Naive Bayes & \textcolor{red}{0.306} & 0.121 & 0.283&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=9)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.398 & 0.153 & 0.369&\\
			 Tied Cov & 0.392 & 0.151 & 0.367&\\
			 Naive Bayes & 0.416 & 0.159 & 0.362&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - no PCA}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.218 & 0.078 & 0.191&\\
			 Tied Cov & 0.208 & 0.078 & 0.189&\\
			 Naive Bayes & 0.813 & 0.586 & 0.847 &\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=10)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.223 & 0.084 & 0.211&\\
			 Tied Cov & 0.212 & 0.082 & 0.207&\\
			 Naive Bayes & 0.279 & 0.103 & 0.254&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=9)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.270 & 0.105 & 0.247&\\
			 Tied Cov & 0.265 & 0.103 & 0.244&\\
			 Naive Bayes & 0.305 & 0.119 & 0.282&\\
			\hline
		\end{tabular}
	\end{table}
\FloatBarrier

A graphical version of the table can be helpful in analyzing results:
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/gaumodels.png}
	\caption{Top: Z-Normalized feautres, Bottom: Gaussianized features}
	\label{gauplots} 
\end{figure}

Some observations:
\begin{itemize}
	\item We can notice that Full-Cov model and Tied-Cov model achieve very good and similar results with
		  slighlty better performances for the tied model.
	\item Gaussianization pre processing does't really help in achieving better results maybe
		  because data are already well distributed according to the Gaussian assumptions
	\item Naive Bayes assumptio doesn't hold well, in particular if PCA is not applied. When 
		  PCA is applied it behaves slightly worse than the other two models. In particular Naive
		  Bayes classfier achieves better results when gaussianization is applied
\end{itemize}

\subsection{Logistic Regression Classifier}
Logistic Regression is a discriminative classification model. Starting from the results obtained
from the Gaussian classifiers we consider the linear decision function obtained from the expression
of the posterior log-likelihood ratio:
\begin{center}
	\begin{math}
		l(x) = log\;\frac{P(C=h_1|x)}{P(C=h_0|x)}=log\;\frac{f_{X|C}(x|h_1)}{f_{X|C}(x|h_0)} + log\;\frac{\pi}{1-\pi} = w^Tx + b
	\end{math}
\end{center}
where $b$ takes into account all the prior information. Given $w$ and $b$ we can compute the
expression for the posterior class probability:
\begin{center}
	\begin{math}
		P(c=h_1|x,w,b)=\frac{e^{(w^Tx + b)}}{1+e^{(w^Tx+b)}}=\sigma(w^Tx+b)
	\end{math}
\end{center}
where $\sigma(x)=\frac{1}{1+e^{-x}}$ is the sigmoid function. Decision rules will be hyperplanes
orthogonal to $w$.
\subsubsection{Linear Logistic Regression}
We are going to look for the minimizer of the function:
\begin{center}
	\begin{math}
		J(w,b) = \frac{\lambda}{2}||w||^2 + \frac{1}{n}\sum_{i=1}^{n}log(1 + e^{-z_i(w^Tx_i+b)})
	\end{math}
\end{center}
where $\lambda$ is an hyperparameter that represents the regularization term (needed to make the 
problem solvable in case of linearly separable classes).

	\begin{table}[ht!]
		\caption{Linear Logistic Regression - 3-fold cross validation}
		\centering
		\begin{tabular}{ |l|l|l|l| }
			\hline
			& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
			\hline
			\multirow{3}{*}{}
			 Linear LR ($\lambda=10^{-3}$)& 0.170 & 0.170 & 0.155\\
			 Linear LR ($\lambda=10^{-5}$)& 0.132 & 0.047 & 0.127\\
			 Linear LR ($\lambda=10^{-6}$)& 0.132 & 0.047 & 0.126\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
			\hline
			\multirow{3}{*}{}
			 Linear LR ($\lambda=10^{-3}$)& 0.299 & 0.113 & 0.263\\
			 Linear LR ($\lambda=10^{-5}$)& 0.297 & 0.114 & 0.263\\
			 Linear LR ($\lambda=10^{-6}$)& 0.297 & 0.114 & 0.262\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=9)}} \\
			\hline
			\multirow{3}{*}{}
			 Linear LR ($\lambda=10^{-3}$)& 0.390 & 0.153 & 0.369\\
			 Linear LR ($\lambda=10^{-5}$)& 0.388 & 0.152 & 0.363\\
			 Linear LR ($\lambda=10^{-6}$)& 0.388 & 0.152 & 0.362\\
			\hline
		\end{tabular}
	\end{table}

\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/dcfplotLLR.png}
	\caption{minDCF for different values of $\lambda$ and different priors}
	\label{dcfLLR} 
\end{figure}
Best performances are obtained for small values of $\lambda$.
\subsubsection{Quadratic Logistic Regression}
Now we are going to train a Quadratic LR model by performing features expansion.
For binary linear LR the separation surfaces are linear decision function as already discussed (and
we obtain the same form as for the Tied Gaussian classifier). By looking instead at the separation surface
obtained through the MVG gaussian classifier we have:
\begin{center}
	\begin{math}
		log\;\frac{P(C=h_1|x)}{P(C=h_0|x)} = x^TAx + b^Tx + c = s(x, A, b, c)
	\end{math}
\end{center}
This expression is quadratic in $x$ but linear in $A$ and $b$. 
We could rewrite it to obtain a decision function that is linear for the expanded features space
but quadratic in the original features space. Features expansion is defined as:
\begin{center}
	\begin{math}
		\Phi(x) = \begin{bmatrix}
					vec(xx^T)\\
					x
				  \end{bmatrix} \;, 
		w= 		 \begin{bmatrix}
					vec(A)\\
					b
				  \end{bmatrix}
	\end{math}
\end{center}
where $vec(X)$ is the operator that stacks the columns of $X$. In this way the posterior log-likelihood is expressed as:
\begin{center}
	\begin{math}
		s(x, w, c) = s^T\phi(x)+c
	\end{math}
\end{center}
We are now going to train the Linear Logistic Regression model using features vectors $\phi(x)$
\begin{table}[ht!]
	\caption{Linear Logistic Regression - 3-fold cross validation}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
		\hline
		\multirow{3}{*}{}
		 Quadratic LR ($\lambda=10^{-3}$)& 0.187 & 0.063 & 0.149\\
		 Quadratic LR ($\lambda=10^{-5}$)& 0.153 & 0.052 & 0.142\\
		 Quadratic LR ($\lambda=10^{-6}$)& 0.150 & 0.053 & 0.141\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
		\hline
		\multirow{3}{*}{}
		 Quadratic LR ($\lambda=10^{-3}$)& 0.299 & 0.111 & 0.241\\
		 Quadratic LR ($\lambda=10^{-5}$)& 0.307 & 0.109 & 0.249\\
		 Quadratic LR ($\lambda=10^{-6}$)& 0.305 & 0.109 & 0.248\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=9)}} \\
		\hline
		\multirow{3}{*}{}
		 Quadratic LR ($\lambda=10^{-3}$)& 0.373 & 0.151 & 0.337\\
		 Quadratic LR ($\lambda=10^{-5}$)& 0.378 & 0.149 & 0.349\\
		 Quadratic LR ($\lambda=10^{-6}$)& 0.378 & 0.149 & 0.350\\
		\hline
	\end{tabular}
\end{table}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/dcfplotQLR.png}
	\caption{minDCF for different values of $\lambda$ and different priors}
	\label{dcfQLR} 
\end{figure}
%------------------------------------------------

\subsection{Subsection}

Nam ante risus, tempor nec lacus ac, congue pretium dui. Donec a nisl est. Integer accumsan mauris eu ex venenatis mollis. Aliquam sit amet ipsum laoreet, mollis sem sit amet, pellentesque quam. Aenean auctor diam eget erat venenatis laoreet. In ipsum felis, tristique eu efficitur at, maximus ac urna. Aenean pulvinar eu lorem eget suscipit. Aliquam et lorem erat. Nam fringilla ante risus, eget convallis nunc pellentesque non. Donec ipsum nisl, consectetur in magna eu, hendrerit pulvinar orci. Mauris porta convallis neque, non viverra urna pulvinar ac. Cras non condimentum lectus. Aliquam odio leo, aliquet vitae tellus nec, imperdiet lacinia turpis. Nam ac lectus imperdiet, luctus nibh a, feugiat urna.

\begin{itemize}
	\item First item in a list 
	\item Second item in a list 
	\item Third item in a list
\end{itemize}

Nunc egestas quis leo sed efficitur. Donec placerat, dui vel bibendum bibendum, tortor ligula auctor elit, aliquet pulvinar leo ante nec tellus. Praesent at vulputate libero, sit amet elementum magna. Pellentesque sodales odio eu ex interdum molestie. Suspendisse lacinia, augue quis interdum posuere, dolor ipsum euismod turpis, sed viverra nibh velit eget dolor. Curabitur consectetur tempus lacus, sit amet luctus mauris interdum vel. Curabitur vehicula convallis felis, eget mattis justo rhoncus eget. Pellentesque et semper lectus.

\begin{description}
	\item[First] This is the first item
	\item[Last] This is the last item
\end{description}

Donec nec nibh sagittis, finibus mauris quis, laoreet augue. Maecenas aliquam sem nunc, vel semper urna hendrerit nec. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Maecenas pellentesque dolor lacus, sit amet pretium felis vestibulum finibus. Duis tincidunt sapien faucibus nisi vehicula tincidunt. Donec euismod suscipit ligula a tempor. Aenean a nulla sit amet magna ullamcorper condimentum. Fusce eu velit vitae libero varius condimentum at sed dui.

%------------------------------------------------

\subsection{Subsection}

In hac habitasse platea dictumst. Etiam ac tortor fermentum, ultrices libero gravida, blandit metus. Vivamus sed convallis felis. Cras vel tortor sollicitudin, vestibulum nisi at, pretium justo. Curabitur placerat elit nunc, sed luctus ipsum auctor a. Nulla feugiat quam venenatis nulla imperdiet vulputate non faucibus lorem. Curabitur mollis diam non leo ullamcorper lacinia.

Morbi iaculis posuere arcu, ut scelerisque sem. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Mauris placerat urna id enim aliquet, non consequat leo imperdiet. Phasellus at nibh ut tortor hendrerit accumsan. Phasellus sollicitudin luctus sapien, feugiat facilisis risus consectetur eleifend. In quis luctus turpis. Nulla sed tellus libero. Pellentesque metus tortor, convallis at tellus quis, accumsan faucibus nulla. Fusce auctor eleifend volutpat. Maecenas vel faucibus enim. Donec venenatis congue congue. Integer sit amet quam ac est aliquam aliquet. Ut commodo justo sit amet convallis scelerisque.

\begin{enumerate}
	\item First numbered item in a list
	\item Second numbered item in a list
	\item Third numbered item in a list
\end{enumerate}

Aliquam elementum nulla at arcu finibus aliquet. Praesent congue ultrices nisl pretium posuere. Nunc vel nulla hendrerit, ultrices justo ut, ultrices sapien. Duis ut arcu at nunc pellentesque consectetur. Vestibulum eget nisl porta, ultricies orci eget, efficitur tellus. Maecenas rhoncus purus vel mauris tincidunt, et euismod nibh viverra. Mauris ultrices tellus quis ante lobortis gravida. Duis vulputate viverra erat, eu sollicitudin dui. Proin a iaculis massa. Nam at turpis in sem malesuada rhoncus. Aenean tempor risus dui, et ultrices nulla rutrum ut. Nam commodo fermentum purus, eget mattis odio fringilla at. Etiam congue et ipsum sed feugiat. Morbi euismod ut purus et tempus. Etiam est ligula, aliquam eget porttitor ut, auctor in risus. Curabitur at urna id dui lobortis pellentesque.


%------------------------------------------------

\section{Section}

\begin{figure}[htpb!]
	\includegraphics[width=\linewidth]{bear.jpg} % Figure image
	\caption{A majestic grizzly bear} % Figure caption
	\label{bear} % Label for referencing with \ref{bear}
\end{figure}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography[title={Bibliography}] % Print the bibliography, section title in curly brackets

%----------------------------------------------------------------------------------------

\end{document}
