%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Article
% LaTeX Template
% Version 2.0 (28/2/17)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[10pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)
\usepackage{placeins}
\usepackage{multirow}
\usepackage{amsmath}
\newcommand{\comment}[1]{}
\input{structure.tex} % Specifies the document structure and loads requires packages

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{MLPR Exam Project: \\Gender Detection} % The article title

\author{
	\authorstyle{Mattia Rosso [s294711]} % Author
	%\newline\newline % Space before institutions
	%\textsuperscript{1}\institution{Universidad Nacional Autónoma de México, Mexico City, Mexico}\\ % Institution 1
	%\textsuperscript{2}\institution{University of Texas at Austin, Texas, United States of America}\\ % Institution 2
	%\textsuperscript{3}\institution{\texttt{LaTeXTemplates.com}} % Institution 3
}

% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}

\date{\today} % Add a date here if you would like one to appear underneath the title block, use \today for the current date, leave empty for no date

%----------------------------------------------------------------------------------------

\begin{document}
\maketitle % Print the title
\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{This project is inteded to show a binary classification 
task on a datased made of 12 continuous observations coming from speking embeddings. 
A speaker embedding represents a smal-dimensional, fixed size representation of an utterance.
Features can be seen as points in the m-dimensional embedding space (and the embeddings
have already been computed). This is a task where classes are balanced both in training and
evaluation set}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Dataset analysis}
\subsection{Training and evaluation sets}
The training set contains:
\begin{itemize}
	\item Training Set: 3000 samples belonging to Male class (Label = 0) and
						3000 samples belonging to Female class (Label = 1).
	\item Evaluation Set: 2000 samples belonging to Male class (Label = 0) and
						  2000 samples belonging to Female class (Label = 1).
\end{itemize}

\subsection{Training set features analysis}
\subsection{Features Statistics}
All the features are contiguous and their main statics can be
showed through a boxplot in figure \ref{boxplot}. 

\subsection{Z-normalization}
A useful operation that can be applied in order to avoid to deal with numerical 
issues and to make data more uniform is to apply Z-normalization as a preprocessing step:\\
\begin{center}
	\begin{math}
		z_{i,j} = \frac{x_{i,j} - \mu_{j}}{\sigma_{j}} \forall x_{i} \in D 
	\end{math}
\end{center}
Where $z_{i,j}$ is the Z-normalized value correspongin to the feature $j$
of sample $i$ while $\mu_{j}$ and $\sigma_{j}$ are, respectively,
the mean and the variance computed over all values for feature $j$.
\comment{
\FloatBarrier
	\begin{table}
		\caption{Features Statistics}
		\centering
		\begin{tabular}{ |l|l|l|l|l| }
			\hline
			\multicolumn{5}{ |c| }{Statistics} \\
			\hline
			Feature & Min & Max & Mean & StdDev \\ \hline
			\multirow{2}{*}{0}
			 & 0 & 0 & 0 & 0 \\
			 & 0 & 0 & 0 & 0  \\ \hline
			\multirow{2}{*}{1}
			 & 0 & 0 & 0 & 0  \\
			 & 0 & 0 & 0 & 0  \\ \hline
			\multirow{2}{*}{2}
			& 0 & 0 & 0 & 0  \\
			& 0 & 0 & 0 & 0  \\ \hline
		\end{tabular}
	\end{table}
\FloatBarrier
}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/boxplot.png}
	\caption{Features boxplot}
	\label{boxplot} 
\end{figure}

\subsection{Features distribution}
By plotting historams for each feature, separated for male and female, it is possible to show
how much each feature follows a gaussian distribution in order to understend whether a pre processing
like gaussianization can be useful to go on with our classification task
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/hist_znorm.png}
	\caption{Z-normalized features distribution}
	\label{hist_znorm} 
\end{figure}

We can notice from figure \ref{hist_znorm} that almost all the features are already 
well-distributed except for features 3, 7, 9. We can apply an additional pre-processing step in
order to make all the features following a gaussian distribution.

\subsubsection{Gaussianization}
Gaussianization is a pre processing step that maps each fature to values whose empirical cumulative
distribution is well approximated by a Gaussian cumulative distribution function. For each
feature $x$ that we want to gaussianize we firstly compute the rank over the dataset:\\
\begin{center}
	\begin{math}
		r(x) = \frac{\sum_{i=1}^{N}\mathbb{I}[x_{i} < x] + 1}{N + 2}
	\end{math}
\end{center}
where $\mathbb{I}$ is the indicator function ($1$ when the condition inside $[ ]$ is true, $0$ 
otherwise). Actually, we are counting how many samples in the dataset $D$ have a greater value
with respect to the feature we are computing the rank on. \\
The next step is to compute the transformed feature as $y = \Phi^{-1}(r(x))$ where $\Phi$ is
the inverse of the cumulative distribution function. 

\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/hist_gau.png}
	\caption{Gaussianized features distribution}
	\label{hist_gau} 
\end{figure}

\subsection{Features correlation}
We can show how much features are correlated by using a heatmap plot showing a darker
color inside cells $[i, j]$ for which it exists an high correlation among feaure $i$ and 
feature $j$. We are going to use the Pearson correlation coefficient to compute the
correlation among feature $X$ and feature $Y$:
\begin{center}
	\begin{math}
		|\frac{Cov(X, Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}|
	\end{math}
\end{center}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/heatmap_znormgau.png}
	\caption{Z-normalized+Gaussianized features correlation: grey the whole dataset, orange F class, 
	blue M class}
	\label{heatmap} 
\end{figure}

Correlation is quite high among most of the features, we can understand that applying PCA
would be meaningful for values of $m$ not below $10$ or $9$ at most. For smaller values of $m$
we would lose important information coming from high-correlated features. 

\section{Dimensionality reduction}
Before proceeding with the classification task i would spend some words on the possible 
dimensioanlity reduction rechniques that can be applied: PCA, LDA.
\subsection{PCA}
As already anticipated, given the heatmap in figure \ref{heatmap} we can observe that PCA
with reasonable values of $m$ can be applied. PCA is a dimensionality reduction technique
that, given a centered dataset $X = {x_{1}, ..., x_{k}}$, it aims to find the subspace of 
$\mathbb{R}^{n}$ that allows to preserve most of the information (the directions with the
highest variance).\\Starting from the sample covariance matrix
\begin{center}
	\begin{math}
		C = \frac{1}{K} \sum_{i}^{}(x_{i} - \bar{x})(x_{i}-\bar{x})^T
	\end{math}
\end{center}
we compute the eigen-decomposition of $C = U\Sigma U^T$ and project the data in the subspace
spanned by the $m$ columns of $U$ corresponding to the $m$ highest eigenvalues:
\begin{center}
	\begin{math}
		y_{i} = P^T(x_{i}-\bar{x})
	\end{math}
\end{center}
In order to select the optimal $m$ we can use a cross-validation approach by inspecting how much
of the total variance of data we are able to retain by using that value of $m$. We exploit the fact
that each eigenvalue correspond to the variance along the corresponding axis and the
eigenvalues are the elements of the diagonal of the matrix $\Sigma$. We selecte $m$ as:
\begin{center}
	\begin{math}
		\min_m\;s.t \frac{\sum_{i}^{m}\sigma_{i}}{\sum_{i}^{n}\sigma_{i}} >= t,\;t \ge 95\%
	\end{math}
\end{center}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/pca.png}
	\caption{3-fold cross validation for PCA impact evaluation}
	\label{pca} 
\end{figure}
We can clearly understand from figure \ref{pca} that values of $m < 9$ would rapidly
decrease the amout of retained variance. We will see better later on how this would 
badly impact the performance of the classifiers

\subsection{LDA}
PCA technique is unsupervised so we have no guarantee of obtainig discriminant directions.
Despite the fact that LDA allows to find at most $C - 1$, where $C$ is the number of classes,
directions, so it makes no sense to apply it as a dimensionality reduction technique, it can
be used as a linear classifier and we can understand it by its definition; LDA mazimizes the
between-class variability over the within-class variability ratio for the transformed samples:
\begin{center}
	\begin{math}
		\mathcal{L}(w) = \frac{s_{B}}{s_{W}} = \max_w \frac{w^TS_{B}w}{w^TS_{W}w}
	\end{math}
\end{center}
It can be proved that optimal solution correspond to the eigenvector of $S_{W}^-1S_{B}$ 
corresponding to the largest eigenvalue. Once that we have estimated $w$ we can project
the test samples over $w$ and assign the class label looking at the score obtained:\\
\begin{center}
	\begin{math}
		f(x)=\left\{
		\begin{array}{ll}
			1, & \mbox{if $x<0$}.\\
			0, & \mbox{otherwise}.
		\end{array}
		\right.
	\end{math}
\end{center}
It will be showed later the result of the classifier
\section{Classification models analysis}
\subsection{Premises}
In the next paragraphs we are going to compare different classification models. 
We will employ a k-fold cross validation technique (with $k=3$) for model evaluation and the
best models will be chosen to train the entire training set for performing a final comparison.
We will consider three types of applications:
\begin{center}
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.1, 1, 1)$\\
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.5, 1, 1)$\\
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.9, 1, 1)$\\
\end{center}
and the target application will be 
\begin{center}
	$(\tilde{\pi}, C_{fp}, C_{fn}) = (0.5, 1, 1)$
\end{center}
We are interested
in selecting the most promising approach and we will infact perform measures in term of minimum
detection cost:
\begin{center}
	\begin{math}
		DCF = \frac{DCF_{u}(\pi_{T}, C_{fn}, C_{fp})}{min(\pi_{T},C_{fn}, (1-\pi_{T}C_{fp}))} = 
			\frac{\pi_{T}C_{fn}P_{fn} + (1-\pi_{T})C_{fp}P_{fp}}{min(\pi_{T},C_{fn}, (1-\pi_{T}C_{fp}))}
	\end{math}
\end{center}
and for $minDCF$ computation we will look for:
\begin{center}
	\begin{math}
		t' = -log(\frac{\tilde{\pi}}{1-\tilde{\pi}})
	\end{math}
\end{center}
that allows us to obtain the lowest possible $DCF$ (as if knew in advance this optimal threshold)
\subsection{Gaussian models}
The first class of models we are going to analyze are the generative gaussian models. Model assumption are that,
given the dataset $X$ we assume that the sample $x_{t}$ is a realization of the R.V. $X_{t}$. A simple
model consists in assuming that our data, given the class, can be described by a Gaussian distribution:
\begin{center}
	\begin{math}
		(X_{t}|C_{t}=c) \sim (X|C=c) \sim \mathcal{N} (x_{t}|\mu_{c},\Sigma_{c})
	\end{math}
\end{center}
We will assign a probabilistic score to each sample in term of the class-posterior 
log-likelihood ratio:
\begin{center}
	\begin{math}
		log\;r(x_{t}) = log\;\frac{P(C = h_{1}|x_{t})}{P(C = h_{0}|x_{t})}
	\end{math}
\end{center}
We can expand this expression by writing:
\begin{center}
	\begin{math}
		log\;r(x_{t}) = log\;\frac{f_{X|C}(x_{t}|h_{1})}{f_{X|C}(x_{t}|h_{0})} + log\;\frac{\pi}{1-\pi}
	\end{math}
\end{center}
While the training phase consists in estimating the model parameters the scoring phase
consists in computing the log-likelihood ratio (first term of the equation) for each sample. 
It will be then compared with  a threshold specific for each application for computing 
the $minDCF$. What differentiate the different Gaussian models is the way how we estimate the
model parameters.
\subsubsection{MVG Gaussian Classifier}
The ML solution to the previous desribed problem is given by the empirical mean and covariance
matrix for each class:
\begin{center}
	$\mu_{c}^* = \frac{1}{N_c}\sum_{i=1}^{N}x_{c,i}$ \\
	$\Sigma_c^* = \frac{1}{N_c}\sum_{i=1}^{N}(x_{c,i}-\mu_c^*)(x_{c,i}-\mu_c^*)$
\end{center}
We will then compute the log densities for each sample by using the estimated model parameters
\subsubsection{Naive Bayes Classifier}
The Naive Bayes assumption simplifies the MVG full covariance model stating that if we knew
that for each class the componenets are approximately independent we can assume that the 
distribution $X|C$ can be factorized over its components. The ML solution to this problem is:
\begin{center}
	$\mu_{c,[j]}^* = \frac{1}{N_c}\sum_{i|c_i=c}^{}x_{i,[j]}$ \\
	$\sigma_{c,[j]}^2 = \frac{1}{N_c}\sum_{i|c_i=c}^{}(x_{i,[j]}-\mu_{c,[j]})^2$
\end{center}
The density of a sample $x$ can be expressed as $\mathcal{N}(x|\mu_c,\Sigma_c)$ where
$\mu_c$ is an array where each element $\mu_{c,[j]}$ is the the mean for each class
for each component while $\Sigma_c$ is a diagonal covariance matrix. The Naive Bayes classifier
corresponds to the MVG full covariance classifier with a diagonal covariance matrix
\subsubsection{Tied Gaussian Classifier}
This model assumes that the covariance matrices of the different class are tied (we consider
only one covariance matrix common to all classes). We are assuming that:
\begin{center}
	$f_{X|C}(x|c) = \mathcal{N} (x|\mu_c, \Sigma)$
\end{center}
so each class has its own mean but the covariance matrix is the same for all the classes. The ML
solution to this problem is:
\begin{center}
	$\mu_{c}^* = \frac{1}{N_c}\sum_{i=1}^{N}x_{c,i}$ \\
	$\Sigma^* = \frac{1}{N}\sum_c{}^{}\sum_{i|c_i=c}^{}(x_{i}-\mu_c^*)(x_{i}-\mu_c^*)$
\end{center}
This model is strongly related to LDA (used as a linear classfication model). By considering 
the binary log-likelihood ratio of the tied model we obtain a linear decision function:
\begin{center}
	$llr(x) = log\;\frac{f_{X|C}(x|h_1)}{f_{X|C}(x|h_0)} = x^Tb + c$ 
\end{center}
where $b$ and $c$ are functions of class means and (tied) covariance matrix. On the other hand,
projecting over the LDA subspace is, up to a scaling factor $k$, given by:
\begin{center}
	$w^Tx = k \cdot x^T\varLambda (\mu_1-\mu_0)$ 
\end{center}
where $\varLambda (\mu_1-\mu_0) = b$. The LDA assumption that all the classes have the same
within class covariance is related to the assumption done for the tied model.\
\subsubsection{Gaussian Models Comparison}
\FloatBarrier
	\begin{table}
		\caption{Gaussian Models}
		\centering
		\begin{tabular}{ |l|l|l|l|l| }
			\hline
			& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ &\\ \hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.128 & 0.048 & 0.125&\\
			 Tied Cov & 0.122 & \textcolor{red}{0.046} & 0.127&\\
			 Naive Bayes & 0.822 & 0.567 & \textcolor{blue}{0.856}&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.303 & 0.115 & 0.267&\\
			 Tied Cov & 0.293 & \textcolor{red}{0.112} & 0.264&\\
			 Naive Bayes & \textcolor{orange}{0.306} & \textcolor{orange}{0.121} & \textcolor{orange}{0.283}&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=9)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.398 & 0.153 & 0.369&\\
			 Tied Cov & 0.392 & \textcolor{red}{0.151} & 0.367&\\
			 Naive Bayes & \textcolor{blue}{0.416} & 0.159 & 0.362&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - no PCA}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.218 & \textcolor{red}{0.078} & 0.191&\\
			 Tied Cov & 0.208 & \textcolor{red}{0.078} & 0.189&\\
			 Naive Bayes & 0.813 & 0.586 & \textcolor{blue}{0.847} &\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=10)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.223 & 0.084 & 0.211&\\
			 Tied Cov & 0.212 & \textcolor{red}{0.082} & 0.207&\\
			 Naive Bayes & \textcolor{blue}{0.279} & \textcolor{orange}{0.103} & 0.254&\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=9)}} &\\
			\hline
			\multirow{3}{*}{}
			 Full Cov & 0.270 & 0.105 & 0.247&\\
			 Tied Cov & 0.265 & \textcolor{red}{0.103} & 0.244&\\
			 Naive Bayes & \textcolor{blue}{0.305} & 0.119 & 0.282&\\
			\hline
		\end{tabular}
	\end{table}
\FloatBarrier

A graphical version of the table can be helpful in analyzing results:
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/gaumodels.png}
	\caption{Top: Z-Normalized feautres, Bottom: Gaussianized features}
	\label{gauplots} 
\end{figure}

Some observations:
\begin{itemize}
	\item We can notice that Full-Cov model and Tied-Cov model achieve very good and similar results with
		  slighlty better performances for the tied model.
	\item Gaussianization pre processing does't really help in achieving better results maybe
		  because data are already well distributed according to the Gaussian assumptions
	\item Naive Bayes assumption doesn't hold really well, in particular if PCA is not applied. When 
		  PCA is applied it has a really good impact only on Naive Bayes model and especialliy if also
		  combined with Gaussianization pre processing.
	\item Regarding Full and Tied models with PCA(m=10) there is no a high performance degradation
		  since the minDCF obtained are still good. For lower values of $m$ the models become less
		  able in taking decisions and the minDCF increases
	\item For the MVG classifiers best performances are achieved by the Tied-Cov classifier with
		  only Z-normalization pre processing and withouth PCA. Really good performacnes are also
		  achieved by the Tied-Cov model trained with Gaussianized
		  fetures and no-PCA (the PCA(m=10) version of this model achieves worse but comparable result w.r.t. to the no-PCA version
		  and it can be selected to try to better avoid overfitting by reducing the features 
		  space and also for reducing computational effort)
\end{itemize}
\begin{table}[ht!]
	\caption{Best Gaussian Models}
	\centering
	\begin{tabular}{ |l|l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ &\\ \hline
		\multicolumn{4}{ |c| }{\bf{Z-Normalized features - no PCA}} &\\
		\hline
		\multirow{1}{*}{}
		 Tied Cov & 0.122 & 0.046 & 0.127&\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Gaussianized features - PCA(m=10)}} &\\
		\hline
		\multirow{1}{*}{}
		 Tied Cov & 0.212 & 0.082 & 0.207&\\
		\hline
	\end{tabular}
\end{table}
\subsection{Logistic Regression Classifier}
Logistic Regression is a discriminative classification model. Starting from the results obtained
from the Gaussian classifiers we consider the linear decision function obtained from the expression
of the posterior log-likelihood ratio:
\begin{center}
	\begin{math}
		l(x) = log\;\frac{P(C=h_1|x)}{P(C=h_0|x)}=log\;\frac{f_{X|C}(x|h_1)}{f_{X|C}(x|h_0)} + log\;\frac{\pi}{1-\pi} = w^Tx + b
	\end{math}
\end{center}
where $b$ takes into account all the prior information. Given $w$ and $b$ we can compute the
expression for the posterior class probability:
\begin{center}
	\begin{math}
		P(c=h_1|x,w,b)=\frac{e^{(w^Tx + b)}}{1+e^{(w^Tx+b)}}=\sigma(w^Tx+b)
	\end{math}
\end{center}
where $\sigma(x)=\frac{1}{1+e^{-x}}$ is the sigmoid function. Decision rules will be hyperplanes
orthogonal to $w$.
\subsubsection{Linear Logistic Regression}
We are going to look for the minimizer of the function:
\begin{center}
	\begin{math}
		J(w,b) = \frac{\lambda}{2}||w||^2 + \frac{1}{n}\sum_{i=1}^{n}log(1 + e^{-z_i(w^Tx_i+b)})
	\end{math}
\end{center}
where $\lambda$ is an hyperparameter that represents the regularization term (needed to make the 
problem solvable in case of linearly separable classes).

	\begin{table}[ht!]
		\caption{Linear Logistic Regression - 3-fold cross validation}
		\centering
		\begin{tabular}{ |l|l|l|l| }
			\hline
			& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
			\hline
			\multirow{3}{*}{}
			 LLR \scriptsize{($\lambda=10^{-3}$)}& 0.170 & 0.170 & 0.155\\
			 LLR \scriptsize{($\lambda=10^{-5}$)}& 0.132 & 0.047 & 0.127\\
			 LLR \scriptsize{($\lambda=10^{-6}$)}& 0.132 & 0.047 & 0.126\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
			\hline
			\multirow{3}{*}{}
			 LLR \scriptsize{($\lambda=10^{-3}$)}& 0.299 & 0.113 & 0.263\\
			 LLR \scriptsize{($\lambda=10^{-5}$)}& 0.297 & 0.114 & 0.263\\
			 LLR \scriptsize{($\lambda=10^{-6}$)}& 0.297 & 0.114 & 0.262\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=9)}} \\
			\hline
			\multirow{3}{*}{}
			 LLR \scriptsize{($\lambda=10^{-3}$)}& 0.390 & 0.153 & 0.369\\
			 LLR \scriptsize{($\lambda=10^{-5}$)}& 0.388 & 0.152 & 0.363\\
			 LLR \scriptsize{($\lambda=10^{-6}$)}& 0.388 & 0.152 & 0.362\\
			\hline
			\multicolumn{4}{ |c| }{\bf{Best Gaussian Models}} \\
			\hline
			\multirow{2}{*}{}
			 Tied Cov \scriptsize{(Z-Norm, no PCA)} & 0.122 & 0.046 & 0.127\\
			 Tied Cov \scriptsize{(Gau, PCA(m=10))} & 0.212 & 0.082 & 0.207\\
			\hline
		\end{tabular}
	\end{table}

\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/dcfplotLLR.png}
	\caption{minDCF for different values of $\lambda$ and different priors}
	\label{dcfLLR} 
\end{figure}
Best performances are obtained for small values of $\lambda$.
\subsubsection{Quadratic Logistic Regression}
Now we are going to train a Quadratic LR model by performing features expansion.
For binary linear LR the separation surfaces are linear decision function as already discussed (and
we obtain the same form as for the Tied Gaussian classifier). By looking instead at the separation surface
obtained through the MVG gaussian classifier we have:
\begin{center}
	\begin{math}
		log\;\frac{P(C=h_1|x)}{P(C=h_0|x)} = x^TAx + b^Tx + c = s(x, A, b, c)
	\end{math}
\end{center}
This expression is quadratic in $x$ but linear in $A$ and $b$. 
We could rewrite it to obtain a decision function that is linear for the expanded features space
but quadratic in the original features space. Features expansion is defined as:
\begin{center}
	\begin{math}
		\Phi(x) = \begin{bmatrix}
					vec(xx^T)\\
					x
				  \end{bmatrix} \;, 
		w= 		 \begin{bmatrix}
					vec(A)\\
					b
				  \end{bmatrix}
	\end{math}
\end{center}
where $vec(X)$ is the operator that stacks the columns of $X$. In this way the posterior log-likelihood is expressed as:
\begin{center}
	\begin{math}
		s(x, w, c) = s^T\phi(x)+c
	\end{math}
\end{center}
We are now going to train the Linear Logistic Regression model using features vectors $\phi(x)$
\begin{table}[ht!]
	\caption{Quadratic Logistic Regression - 3-fold cross validation}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
		\hline
		\multirow{3}{*}{}
		 QLR \scriptsize{($\lambda=10^{-3}$)}& 0.187 & 0.063 & 0.149\\
		 QLR \scriptsize{($\lambda=10^{-5}$)}& 0.153 & 0.052 & 0.142\\
		 QLR \scriptsize{($\lambda=10^{-6}$)}& 0.150 & 0.053 & 0.141\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
		\hline
		\multirow{3}{*}{}
		 QLR \scriptsize{($\lambda=10^{-3}$)}& 0.299 & 0.111 & 0.241\\
		 QLR \scriptsize{($\lambda=10^{-5}$)}& 0.307 & 0.109 & 0.249\\
		 QLR \scriptsize{($\lambda=10^{-6}$)}& 0.305 & 0.109 & 0.248\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=9)}} \\
		\hline
		\multirow{3}{*}{}
		 QLR \scriptsize{($\lambda=10^{-3}$)}& 0.373 & 0.151 & 0.337\\
		 QLR \scriptsize{($\lambda=10^{-5}$)}& 0.378 & 0.149 & 0.349\\
		 QLR \scriptsize{($\lambda=10^{-6}$)}& 0.378 & 0.149 & 0.350\\
		\hline
	\end{tabular}
\end{table}
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/dcfplotQLR.png}
	\caption{minDCF for different values of $\lambda$ and different priors}
	\label{dcfQLR} 
\end{figure}
\subsection{SVM Classifier}
\subsubsection{Linear SVM}
Support Vector Machines are linear classifiers that look for maximum margin separation hyperplanes
The primal formulation consists in minimizing:
\begin{center}
	\begin{math}
		J(w,b)=\frac{1}{2}||w^2|| + C\sum_{i=1}^{N}max(0, 1-z_i(w^Tx_i+b))
	\end{math}
\end{center}
with $n$ the number of trainig samples and $C$ is an hyperparameter.\\
We have also considered the dual formulation to solve the SVM problem that consists in maximizing:
\begin{center}
	\begin{math}
		J^D(\alpha)=-\frac{1}{2}\alpha^TH\alpha + \alpha^T\textbf{1} \; s.t.\; 0 \le \alpha_i \le C, \forall i\in \{1,...,n\}\;,\;\sum_{i=1}^{n}\alpha_iz_i=0
	\end{math}
\end{center}
where $\textbf{1}$ is a $n$-dimensional vector of ones and $H$ is the matrix
whose elements are $H_ij = z_iz_jx_i^Tx_j$.\\
The relation between the primal and the dual formulation(between the maximizer values $w^*$ and $\alpha^*$) is:
\begin{center}
	$w^*=\sum_{i=1}^{n}\alpha_i^*z_ix_i$
\end{center}
and the optimal bias $b$ can be computed considering a sample $x_i$ that lies on the margin: $z_i(w^{*T}x_i+b^*)=1$
To be able to computationally solve the problem
we need to modify the primal formulation as:
\begin{center}
	\begin{math}
		\hat{J}(\hat{w})=\frac{1}{2}||\hat{w}||^2 + C\sum_{i=1}^{N}max(0, 1-z_i(\hat{w}^T\hat{x}_i))
	\end{math}
\end{center}
where $\hat{x}_i = \begin{bmatrix} x_i\\ 1 \end{bmatrix}$ and $\hat{w} = \begin{bmatrix} w\\ b \end{bmatrix}$.\\
The scoring rule $\hat{w}^T\hat{x}_i = w^Tx + b$ has the same form of the original formulation but
we are also regularizing the norm of $\hat{w}: ||\hat{w}||^2 = ||w||^2 + b^2$ and we use a mapping
$\hat{x}_i = \begin{bmatrix} x_i\\ K \end{bmatrix}$ to mitigate the fact that by regularizing  the bias
term we could obtain sub-optimal results. The bigger is $K$ the lower the regularization effect of $b$
becomes weaker and the dual formulation becomes harder to solve. According to the modification done to
the primal formulation we also modify the dual formulation as:
\begin{center}
	\begin{math}
		J^D(\alpha)=-\frac{1}{2}\alpha^T\hat{H}\alpha + \alpha^T\textbf{1} \; s.t.\; 0 \le \alpha_i \le C, \forall i\in \{1,...,n\}
	\end{math}
\end{center}
where the equality constraint dissapeared (the one that L-BFGS was not able to incorporate) and the
matrix $\hat{H}$ can be computed as $\hat{H}_{i,j}=z_iz_j\hat{x}_i^T\hat{x}_j$
\begin{figure}[ht!]
	\includegraphics[width=\linewidth]{./Pictures/FeaturesAnalysis/LSVM.png}
	\caption{Linear SVM - minDCF for different values of $C$ and different priors}
	\label{lsvm} 
\end{figure}
\begin{table}[ht!]
	\caption{Linear SVM - 3-fold cross validation}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
		\hline
		\multirow{2}{*}{}
		 Linear SVM \scriptsize{($C=0.1$)}& 0.158 & 0.052 & 0.141\\
		 Linear SVM \scriptsize{($C=1$)}& 0.129 & 0.047 & 0.130\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
		\hline
		\multirow{2}{*}{}
		Linear SVM \scriptsize{($C=0.1$)}& 0.295 & 0.114 & 0.259\\
		Linear SVM \scriptsize{($C=1$)}& 0.301 & 0.113 & 0.267\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=9)}} \\
		\hline
		\multirow{2}{*}{}
		Linear SVM \scriptsize{($C=0.1$)}& 0.295 & 0.114 & 0.259\\
		Linear SVM \scriptsize{($C=1$)}& 0.301 & 0.113 & 0.267\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Best Gaussian Models}} \\
		\hline
		\multirow{2}{*}{}
		Tied Cov \scriptsize{(Z-Norm, no PCA)} & 0.122 & 0.046 & 0.127\\
		Tied Cov \scriptsize{(Gau, PCA(m=10))} & 0.212 & 0.082 & 0.207\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Best Logistic Regression Models}} \\
		\hline
		\multirow{2}{*}{}
		Tied Cov \scriptsize{(Z-Norm, no PCA)} & 0.122 & 0.046 & 0.127\\
		Tied Cov \scriptsize{(Gau, PCA(m=10))} & 0.212 & 0.082 & 0.207\\
		\hline
	\end{tabular}
\end{table}
\subsubsection{Kernel SVM}
SVMs allow for non-linear classification through an implicit expansion of the features in a higher
dimensional space. In contrast with Quadratic Logistic Regression classifier we don't have to compute
an explicit expansion of the features space, it is sufficient to be able to compute the scalar product between
the expanded features $k(x_1, x_2) = \phi(x_1)^T\phi(x_2)$ and $k$ is the kernel function. We have to 
replace $\hat{H}$ with $\hat{H} = z_iz_jk(z_1,x_2)$. We are going to try two types of kernel: polynomial and rbf.

%--
\subsection{GMM Classifier}
The last model we take into account is a generative model. We are going to train a GMM
over the samples of each class 
\begin{table}[ht!]
	\caption{GMM - 3-fold cross validation}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		& $\tilde{\pi}=0.1$ & $\tilde{\pi}=0.5$ & $\tilde{\pi}=0.9$ \\ \hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - no PCA}} \\
		\hline
		\multirow{6}{*}{}
		 GMM Full (8 comp.)& 0.109 & 0.040 & 0.102\\
		 GMM Full (16 comp.)& 0.136 & 0.053 & 0.132\\
		 GMM Tied (8 comp.)& 0.099 & 0.031 & 0.075\\
		 GMM Tied (16 comp.)& 0.094 & 0.030 & 0.075\\
		 GMM Diag (8 comp.)& 0.214 & 0.085 & 0.215\\
		 GMM Diag (16 comp.)& 0.224 & 0.084 & 0.231\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=10)}} \\
		\hline
		\multirow{6}{*}{}
		GMM Full (8 comp.)& 0.225 & 0.082 & 0.212\\
		GMM Full (16 comp.)& 0.255 & 0.091 & 0.214\\
		GMM Tied (8 comp.)& 0.204 & 0.072 & 0.186\\
		GMM Tied (16 comp.)& 0.214 & 0.073 & 0.187\\
		GMM Diag (8 comp.)& 0.310 & 0.108 & 0.260\\
		GMM Diag (16 comp.)& 0.354 & 0.116 & 0.309\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Z-normalized features - PCA(m=9)}} \\
		\hline
		\multirow{6}{*}{}
		GMM Full (8 comp.)& 0.389 & 0.150 & 0.361\\
		GMM Full (16 comp.)& 0.409 & 0.165 & 0.404\\
		GMM Tied (8 comp.)& 0.347 & 0.133 & 0.328\\
		GMM Tied (16 comp.)& 0.371 & 0.136 & 0.338\\
		GMM Diag (8 comp.)& 0.422 & 0.170 & 0.424\\
		GMM Diag (16 comp.)& 0.466 & 0.183 & 0.444\\
		\hline
	\end{tabular}
\end{table}
%------------------------------------------------

\subsection{Subsection}

Nam ante risus, tempor nec lacus ac, congue pretium dui. Donec a nisl est. Integer accumsan mauris eu ex venenatis mollis. Aliquam sit amet ipsum laoreet, mollis sem sit amet, pellentesque quam. Aenean auctor diam eget erat venenatis laoreet. In ipsum felis, tristique eu efficitur at, maximus ac urna. Aenean pulvinar eu lorem eget suscipit. Aliquam et lorem erat. Nam fringilla ante risus, eget convallis nunc pellentesque non. Donec ipsum nisl, consectetur in magna eu, hendrerit pulvinar orci. Mauris porta convallis neque, non viverra urna pulvinar ac. Cras non condimentum lectus. Aliquam odio leo, aliquet vitae tellus nec, imperdiet lacinia turpis. Nam ac lectus imperdiet, luctus nibh a, feugiat urna.

\begin{itemize}
	\item First item in a list 
	\item Second item in a list 
	\item Third item in a list
\end{itemize}

Nunc egestas quis leo sed efficitur. Donec placerat, dui vel bibendum bibendum, tortor ligula auctor elit, aliquet pulvinar leo ante nec tellus. Praesent at vulputate libero, sit amet elementum magna. Pellentesque sodales odio eu ex interdum molestie. Suspendisse lacinia, augue quis interdum posuere, dolor ipsum euismod turpis, sed viverra nibh velit eget dolor. Curabitur consectetur tempus lacus, sit amet luctus mauris interdum vel. Curabitur vehicula convallis felis, eget mattis justo rhoncus eget. Pellentesque et semper lectus.

\begin{description}
	\item[First] This is the first item
	\item[Last] This is the last item
\end{description}

Donec nec nibh sagittis, finibus mauris quis, laoreet augue. Maecenas aliquam sem nunc, vel semper urna hendrerit nec. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Maecenas pellentesque dolor lacus, sit amet pretium felis vestibulum finibus. Duis tincidunt sapien faucibus nisi vehicula tincidunt. Donec euismod suscipit ligula a tempor. Aenean a nulla sit amet magna ullamcorper condimentum. Fusce eu velit vitae libero varius condimentum at sed dui.

%------------------------------------------------

\subsection{Subsection}

In hac habitasse platea dictumst. Etiam ac tortor fermentum, ultrices libero gravida, blandit metus. Vivamus sed convallis felis. Cras vel tortor sollicitudin, vestibulum nisi at, pretium justo. Curabitur placerat elit nunc, sed luctus ipsum auctor a. Nulla feugiat quam venenatis nulla imperdiet vulputate non faucibus lorem. Curabitur mollis diam non leo ullamcorper lacinia.

Morbi iaculis posuere arcu, ut scelerisque sem. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Mauris placerat urna id enim aliquet, non consequat leo imperdiet. Phasellus at nibh ut tortor hendrerit accumsan. Phasellus sollicitudin luctus sapien, feugiat facilisis risus consectetur eleifend. In quis luctus turpis. Nulla sed tellus libero. Pellentesque metus tortor, convallis at tellus quis, accumsan faucibus nulla. Fusce auctor eleifend volutpat. Maecenas vel faucibus enim. Donec venenatis congue congue. Integer sit amet quam ac est aliquam aliquet. Ut commodo justo sit amet convallis scelerisque.

\begin{enumerate}
	\item First numbered item in a list
	\item Second numbered item in a list
	\item Third numbered item in a list
\end{enumerate}

Aliquam elementum nulla at arcu finibus aliquet. Praesent congue ultrices nisl pretium posuere. Nunc vel nulla hendrerit, ultrices justo ut, ultrices sapien. Duis ut arcu at nunc pellentesque consectetur. Vestibulum eget nisl porta, ultricies orci eget, efficitur tellus. Maecenas rhoncus purus vel mauris tincidunt, et euismod nibh viverra. Mauris ultrices tellus quis ante lobortis gravida. Duis vulputate viverra erat, eu sollicitudin dui. Proin a iaculis massa. Nam at turpis in sem malesuada rhoncus. Aenean tempor risus dui, et ultrices nulla rutrum ut. Nam commodo fermentum purus, eget mattis odio fringilla at. Etiam congue et ipsum sed feugiat. Morbi euismod ut purus et tempus. Etiam est ligula, aliquam eget porttitor ut, auctor in risus. Curabitur at urna id dui lobortis pellentesque.


%------------------------------------------------

\section{Section}

\begin{figure}[htpb!]
	\includegraphics[width=\linewidth]{bear.jpg} % Figure image
	\caption{A majestic grizzly bear} % Figure caption
	\label{bear} % Label for referencing with \ref{bear}
\end{figure}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography[title={Bibliography}] % Print the bibliography, section title in curly brackets

%----------------------------------------------------------------------------------------

\end{document}
