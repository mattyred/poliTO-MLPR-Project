{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9e8754-a03f-480c-8417-0caada2b83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "import scipy.optimize\n",
    "import sklearn.datasets\n",
    "class LinearLogisticRegression:\n",
    "\n",
    "    def __init__(self, lbd, prior_weighted=False, prior=0.5):\n",
    "        self.lbd = lbd\n",
    "        self.prior_weighted = prior_weighted\n",
    "        self.prior = prior\n",
    "\n",
    "\n",
    "    def __compute_zi(self, ci):\n",
    "        return 2 * ci - 1\n",
    "\n",
    "    def __logreg_obj(self, v):\n",
    "        w, b = v[0:-1], v[-1]\n",
    "        z = 2 * self.Ltrain - 1\n",
    "        exp = (np.dot(w.T, self.Dtrain) + b)\n",
    "        reg = 0.5 * self.lbd * np.linalg.norm(w) ** 2\n",
    "        avg_risk = (np.logaddexp(0, -exp * z)).mean()\n",
    "        return reg + avg_risk\n",
    "\n",
    "    def __logreg_obj_prior_weighted(self, v):\n",
    "        w, b = v[0:-1], v[-1]\n",
    "        z = 2 * self.Ltrain - 1\n",
    "        reg = 0.5 * self.lbd * np.linalg.norm(w) ** 2\n",
    "        exp = (np.dot(w.T, self.Dtrain) + b)\n",
    "        avg_risk_0 = np.logaddexp(0, -exp[self.Ltrain == 0] * z[self.Ltrain == 0]).mean()*(1-self.prior)\n",
    "        avg_risk_1 = np.logaddexp(0, -exp[self.Ltrain == 1] * z[self.Ltrain == 1]).mean() * self.prior\n",
    "        return reg + avg_risk_0 + avg_risk_1\n",
    "    def train(self, Dtrain, Ltrain):\n",
    "        self.Dtrain = Dtrain\n",
    "        self.Ltrain = Ltrain\n",
    "        self.F = Dtrain.shape[0]  # dimensionality of features space\n",
    "        self.K = len(set(Ltrain))  # number of classes\n",
    "        self.N = Dtrain.shape[1]\n",
    "        obj_function = self.__logreg_obj if self.prior_weighted is False else self.__logreg_obj_prior_weighted\n",
    "        self.x, f, d = scipy.optimize.fmin_l_bfgs_b(func=obj_function,\n",
    "                                                    x0=np.zeros(self.Dtrain.shape[0] + 1),\n",
    "                                                    approx_grad=True,\n",
    "                                                    iprint=0)\n",
    "\n",
    "        \"\"\"                                            \n",
    "        print('Point of minimum: %s' % (self.x))\n",
    "        print('Value of the minimum: %s' % (f))\n",
    "        print('Number of iterations: %s' % (d['funcalls']))\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def predict(self, Dtest, labels=True):\n",
    "        w, b = self.x[0:-1], self.x[-1]\n",
    "        S = np.zeros((Dtest.shape[1]))\n",
    "        for i in range(Dtest.shape[1]):\n",
    "            xi = Dtest[:, i:i + 1]\n",
    "            s = np.dot(w.T, xi) + b\n",
    "            S[i] = s\n",
    "        if labels:\n",
    "            LP = S > 0\n",
    "            return LP\n",
    "        else:\n",
    "            return S\n",
    "\n",
    "class QuadraticLogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __compute_zi(self, ci):\n",
    "        return 2 * ci - 1\n",
    "\n",
    "    def __logreg_obj(self, v):  # still works if DTR is one sample only? yes but it must be of shape (4,1)\n",
    "        w, b = v[0:-1], v[-1]\n",
    "        J = self.lbd / 2 * (np.linalg.norm(w) ** 2)\n",
    "        summary = 0\n",
    "        for i in range(self.N):\n",
    "            xi = self.Dtrain[:, i:i + 1]\n",
    "            ci = self.Ltrain[i]\n",
    "            zi = self.__compute_zi(ci)\n",
    "            summary += np.logaddexp(0, -zi * (np.dot(w.T, xi) + b))\n",
    "        J += (1 / self.N) * summary\n",
    "        return J\n",
    "\n",
    "    def train(self, Dtrain, Ltrain, lbd, maxiter):\n",
    "        self.Dtrain = Dtrain\n",
    "        self.Ltrain = Ltrain\n",
    "        self.lbd = lbd\n",
    "        self.maxiter = maxiter\n",
    "        self.F = Dtrain.shape[0]  # dimensionality of features space\n",
    "        self.K = len(set(Ltrain))  # number of classes\n",
    "        self.N = Dtrain.shape[1]\n",
    "        DTR_ext = numpy.hstack([self.__expandFeatures(self.Dtrain[:, i]) for i in range(self.Dtrain.shape[1])])\n",
    "        self.x, f, d = scipy.optimize.fmin_l_bfgs_b(func=self.__logreg_obj, \n",
    "                                                    x0=numpy.zeros(DTR_ext.shape[0] + 1), \n",
    "                                                    approx_grad = True, \n",
    "                                                    factr=1.0)\n",
    "        print('Point of minimum: %s' % (self.x))\n",
    "        print('Value of the minimum: %s' % (f))\n",
    "        print('Number of iterations: %s' % (d['funcalls']))\n",
    "        return self    \n",
    "    \n",
    "    def __expandFeatures(x):\n",
    "        x = utils.mcol(x)\n",
    "        expX = utils.mcol(numpy.dot(x, x.T))\n",
    "        return numpy.vstack([expX, x])\n",
    "\n",
    "    def predict(self, Dtest):\n",
    "        DTE_ext = numpy.hstack([self.__expandFeatures(Dtest[:, i]) for i in range(Dtest.shape[1])])\n",
    "        w, b = utils.mcol(self.x[0:-1]), self.x[-1]\n",
    "        scores = numpy.dot(w.T, DTE_ext) + b\n",
    "        return scores[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a299b5-d1e4-4cc4-b2da-2b75c6cda2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iris_binary():\n",
    "    D, L = sklearn.datasets.load_iris()['data'].T, sklearn.datasets.load_iris()['target']\n",
    "    D = D[:, L != 0] # remove setosa from D\n",
    "    L = L[L!=0] # remove setosa from L\n",
    "    L[L==2] = 0 # We assign label 0 to virginica (was label 2)\n",
    "    return D, L\n",
    "def split_db_2to1(D, L, seed=0):\n",
    "    nTrain = int(D.shape[1]*2.0/3.0) # 2/3 of the dataset D are used for training, 1/3 for validation\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1]) # take a random array of 150 elements, each element is 0<x<=149 (np.arange(150))\n",
    "    idxTrain = idx[0:nTrain] # first 100 are indices of training samples\n",
    "    idxTest = idx[nTrain:] # remaining 50 are indices of validation samples\n",
    "    DTR = D[:, idxTrain] # D for training\n",
    "    DTE = D[:, idxTest] # D for validation\n",
    "    LTR = L[idxTrain] # L for training\n",
    "    LTE = L[idxTest] # L for validation\n",
    "    return (DTR, LTR), (DTE, LTE)\n",
    "\n",
    "DT, LT = load_iris_binary()\n",
    "(DTR, LTR), (DTE, LTE) = split_db_2to1(DT, LT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4c549cd-dd38-4bc5-a8a5-77cc4ad876c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate:  8.823529411764708\n"
     ]
    }
   ],
   "source": [
    "lbd = 10**-3\n",
    "maxiter=100\n",
    "model = LinearLogisticRegression(lbd, prior_weighted=True, prior=0.5)\n",
    "labs = model.train(DTR, LTR).predict(DTE, labels=True)\n",
    "print('Error rate: ', (1 - (sum(labs == LTE) / len(LTE))) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70261e-33fd-45bf-9b14-e9a5286e912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbd = 10**-3\n",
    "maxiter=100\n",
    "model = QuadraticLogisticRegression()\n",
    "labs = model.train(DTR, LTR, lbd, maxiter).predict(DTE, labels=True)\n",
    "print('Error rate: ', 1 - (sum(labs == LTE) / len(LTE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be71970e-41b9-4138-8d69-f8d121e25682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-06, 1.01010101e+03, 2.02020202e+03, 3.03030303e+03,\n",
       "       4.04040404e+03, 5.05050505e+03, 6.06060606e+03, 7.07070707e+03,\n",
       "       8.08080808e+03, 9.09090909e+03, 1.01010101e+04, 1.11111111e+04,\n",
       "       1.21212121e+04, 1.31313131e+04, 1.41414141e+04, 1.51515152e+04,\n",
       "       1.61616162e+04, 1.71717172e+04, 1.81818182e+04, 1.91919192e+04,\n",
       "       2.02020202e+04, 2.12121212e+04, 2.22222222e+04, 2.32323232e+04,\n",
       "       2.42424242e+04, 2.52525253e+04, 2.62626263e+04, 2.72727273e+04,\n",
       "       2.82828283e+04, 2.92929293e+04, 3.03030303e+04, 3.13131313e+04,\n",
       "       3.23232323e+04, 3.33333333e+04, 3.43434343e+04, 3.53535354e+04,\n",
       "       3.63636364e+04, 3.73737374e+04, 3.83838384e+04, 3.93939394e+04,\n",
       "       4.04040404e+04, 4.14141414e+04, 4.24242424e+04, 4.34343434e+04,\n",
       "       4.44444444e+04, 4.54545455e+04, 4.64646465e+04, 4.74747475e+04,\n",
       "       4.84848485e+04, 4.94949495e+04, 5.05050505e+04, 5.15151515e+04,\n",
       "       5.25252525e+04, 5.35353535e+04, 5.45454545e+04, 5.55555556e+04,\n",
       "       5.65656566e+04, 5.75757576e+04, 5.85858586e+04, 5.95959596e+04,\n",
       "       6.06060606e+04, 6.16161616e+04, 6.26262626e+04, 6.36363636e+04,\n",
       "       6.46464646e+04, 6.56565657e+04, 6.66666667e+04, 6.76767677e+04,\n",
       "       6.86868687e+04, 6.96969697e+04, 7.07070707e+04, 7.17171717e+04,\n",
       "       7.27272727e+04, 7.37373737e+04, 7.47474747e+04, 7.57575758e+04,\n",
       "       7.67676768e+04, 7.77777778e+04, 7.87878788e+04, 7.97979798e+04,\n",
       "       8.08080808e+04, 8.18181818e+04, 8.28282828e+04, 8.38383838e+04,\n",
       "       8.48484848e+04, 8.58585859e+04, 8.68686869e+04, 8.78787879e+04,\n",
       "       8.88888889e+04, 8.98989899e+04, 9.09090909e+04, 9.19191919e+04,\n",
       "       9.29292929e+04, 9.39393939e+04, 9.49494949e+04, 9.59595960e+04,\n",
       "       9.69696970e+04, 9.79797980e+04, 9.89898990e+04, 1.00000000e+05])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(10**-6, 10**5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eb702c9-e35f-44eb-81a2-b6acd068c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLinearSVM(DTR, LTR, K, C, DTE, p = 0):\n",
    "    Z = LTR * 2.0 - 1.0\n",
    "    X_hat = np.vstack([DTR, K * np.ones((1, DTR.shape[1]))])\n",
    "    G = np.dot(X_hat.T, X_hat)\n",
    "    H_hat = Z.reshape(-1,1) * Z.reshape(1,-1) * G\n",
    "    empP = (LTR == 1).sum()/len(LTR)\n",
    "    alphaBounds = np.array([(0, C)] * LTR.shape[0])\n",
    "    \n",
    "    if p != 0:\n",
    "        alphaBounds[LTR == 1] = (0, C*p/empP)\n",
    "        alphaBounds[LTR == 0] = (0, C*(1-p)/(1-empP))\n",
    "    \n",
    "    def computeDualLoss(alpha):   \n",
    "        return 0.5 * np.dot(np.dot(alpha.reshape(1,-1), H_hat), alpha) - alpha.sum(), np.dot(H_hat, alpha) - 1\n",
    "        \n",
    "    def computePrimalFromDual(alpha):\n",
    "        w_hat = np.dot(alpha, (Z * X_hat).T)\n",
    "        w = w_hat[:-1]\n",
    "        b = w_hat[-1::]                \n",
    "        return w_hat, w, b\n",
    "    \n",
    "    def computeSVMScore(w, b):\n",
    "        return np.dot(w.T, DTE) + b*K\n",
    "    \n",
    "    alphaStar, x, y = scipy.optimize.fmin_l_bfgs_b(\n",
    "        computeDualLoss, \n",
    "        np.zeros(DTR.shape[1]), \n",
    "        bounds = alphaBounds, \n",
    "        factr=1.0,\n",
    "        maxfun=100000,\n",
    "        maxiter=100000)\n",
    "\n",
    "    w_hat, w, b = computePrimalFromDual(alphaStar)    \n",
    "    score = computeSVMScore(w, b)\n",
    "    # print(f'Elapsed {time.time() - t} seconds')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84ab3072-92c6-4f12-b4cc-0fa34ef900b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.65013479, -1.09932291, -0.60630051, -1.14728007, -0.65037293,\n",
       "       -0.51364522, -0.72628327, -1.10755269, -1.23297549, -1.01174305,\n",
       "       -0.98934839, -1.32817439, -1.39589771, -1.01111484, -0.74778862,\n",
       "       -0.78553736, -1.24814348, -0.74448234, -0.7347432 , -1.20625326,\n",
       "       -1.21583497, -0.63719537, -0.99415472, -1.26680952, -1.12868295,\n",
       "       -0.833613  , -0.67290205, -0.97786307, -0.74668182, -1.07743798,\n",
       "       -1.38828666, -1.25207186, -0.73230558, -0.76385711])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLinearSVM(DTR, LTR, 1, 0.1, DTE, p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c916d38-fef2-4069-9f40-7441a1a37198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
