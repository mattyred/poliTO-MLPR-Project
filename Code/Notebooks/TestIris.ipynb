{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e9e8754-a03f-480c-8417-0caada2b83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "import scipy.optimize\n",
    "\n",
    "class LinearLogisticRegression:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __compute_zi(self, ci):\n",
    "        return 2 * ci - 1\n",
    "\n",
    "    def __logreg_obj(self, v):  # still works if DTR is one sample only? yes but it must be of shape (4,1)\n",
    "        w, b = v[0:-1], v[-1]\n",
    "        J = self.lbd / 2 * (np.linalg.norm(w) ** 2)\n",
    "        summary = 0\n",
    "        for i in range(self.N):\n",
    "            xi = self.Dtrain[:, i:i + 1]\n",
    "            ci = self.Ltrain[i]\n",
    "            zi = self.__compute_zi(ci)\n",
    "            summary += np.logaddexp(0, -zi * (np.dot(w.T, xi) + b))\n",
    "        J += (1 / self.N) * summary\n",
    "        return J\n",
    "\n",
    "    def train(self, Dtrain, Ltrain, lbd, maxiter):\n",
    "        self.Dtrain = Dtrain\n",
    "        self.Ltrain = Ltrain\n",
    "        self.lbd = lbd\n",
    "        self.maxiter = maxiter\n",
    "        self.F = Dtrain.shape[0]  # dimensionality of features space\n",
    "        self.K = len(set(Ltrain))  # number of classes\n",
    "        self.N = Dtrain.shape[1]\n",
    "        self.x, f, d = scipy.optimize.fmin_l_bfgs_b(func=self.__logreg_obj,\n",
    "                                                    x0=np.zeros(self.Dtrain.shape[0] + 1),\n",
    "                                                    approx_grad=True,\n",
    "                                                    iprint=0,\n",
    "                                                    maxiter=self.maxiter)\n",
    "        print('Point of minimum: %s' % (self.x))\n",
    "        print('Value of the minimum: %s' % (f))\n",
    "        print('Number of iterations: %s' % (d['funcalls']))\n",
    "        return self\n",
    "\n",
    "    def predict(self, Dtest, labels=True):\n",
    "        w, b = self.x[0:-1], self.x[-1]\n",
    "        S = np.zeros((Dtest.shape[1]))\n",
    "        for i in range(Dtest.shape[1]):\n",
    "            xi = Dtest[:, i:i + 1]\n",
    "            s = np.dot(w.T, xi) + b\n",
    "            S[i] = s\n",
    "        if labels:\n",
    "            LP = S > 0\n",
    "            return LP\n",
    "        else:\n",
    "            return S\n",
    "class QuadraticLogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __compute_zi(self, ci):\n",
    "        return 2 * ci - 1\n",
    "\n",
    "    def __logreg_obj(self, v):  # still works if DTR is one sample only? yes but it must be of shape (4,1)\n",
    "        w, b = v[0:-1], v[-1]\n",
    "        J = self.lbd / 2 * (np.linalg.norm(w) ** 2)\n",
    "        summary = 0\n",
    "        for i in range(self.N):\n",
    "            xi = self.Dtrain[:, i:i + 1]\n",
    "            ci = self.Ltrain[i]\n",
    "            zi = self.__compute_zi(ci)\n",
    "            summary += np.logaddexp(0, -zi * (np.dot(w.T, xi) + b))\n",
    "        J += (1 / self.N) * summary\n",
    "        return J\n",
    "\n",
    "    def train(self, Dtrain, Ltrain, lbd, maxiter):\n",
    "        self.Dtrain = Dtrain\n",
    "        self.Ltrain = Ltrain\n",
    "        self.lbd = lbd\n",
    "        self.maxiter = maxiter\n",
    "        self.F = Dtrain.shape[0]  # dimensionality of features space\n",
    "        self.K = len(set(Ltrain))  # number of classes\n",
    "        self.N = Dtrain.shape[1]\n",
    "        DTR_ext = numpy.hstack([self.__expandFeatures(self.Dtrain[:, i]) for i in range(self.Dtrain.shape[1])])\n",
    "        self.x, f, d = scipy.optimize.fmin_l_bfgs_b(func=self.__logreg_obj, \n",
    "                                                    x0=numpy.zeros(DTR_ext.shape[0] + 1), \n",
    "                                                    approx_grad = True, \n",
    "                                                    factr=1.0)\n",
    "        print('Point of minimum: %s' % (self.x))\n",
    "        print('Value of the minimum: %s' % (f))\n",
    "        print('Number of iterations: %s' % (d['funcalls']))\n",
    "        return self    \n",
    "    \n",
    "    def __expandFeatures(x):\n",
    "        x = utils.mcol(x)\n",
    "        expX = utils.mcol(numpy.dot(x, x.T))\n",
    "        return numpy.vstack([expX, x])\n",
    "\n",
    "    def predict(self, Dtest):\n",
    "        DTE_ext = numpy.hstack([self.__expandFeatures(Dtest[:, i]) for i in range(Dtest.shape[1])])\n",
    "        w, b = utils.mcol(self.x[0:-1]), self.x[-1]\n",
    "        scores = numpy.dot(w.T, DTE_ext) + b\n",
    "        return scores[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52a299b5-d1e4-4cc4-b2da-2b75c6cda2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iris_binary():\n",
    "    D, L = sklearn.datasets.load_iris()['data'].T, sklearn.datasets.load_iris()['target']\n",
    "    D = D[:, L != 0] # remove setosa from D\n",
    "    L = L[L!=0] # remove setosa from L\n",
    "    L[L==2] = 0 # We assign label 0 to virginica (was label 2)\n",
    "    return D, L\n",
    "def split_db_2to1(D, L, seed=0):\n",
    "    nTrain = int(D.shape[1]*2.0/3.0) # 2/3 of the dataset D are used for training, 1/3 for validation\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1]) # take a random array of 150 elements, each element is 0<x<=149 (np.arange(150))\n",
    "    idxTrain = idx[0:nTrain] # first 100 are indices of training samples\n",
    "    idxTest = idx[nTrain:] # remaining 50 are indices of validation samples\n",
    "    DTR = D[:, idxTrain] # D for training\n",
    "    DTE = D[:, idxTest] # D for validation\n",
    "    LTR = L[idxTrain] # L for training\n",
    "    LTE = L[idxTest] # L for validation\n",
    "    return (DTR, LTR), (DTE, LTE)\n",
    "DT, LT = load_iris_binary()\n",
    "(DTR, LTR), (DTE, LTE) = split_db_2to1(D, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4c549cd-dd38-4bc5-a8a5-77cc4ad876c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point of minimum: [ 1.72972493  0.98292107 -4.54959632 -7.12481584 20.95265108]\n",
      "Value of the minimum: [0.1100009]\n",
      "Number of iterations: 258\n",
      "Error rate:  0.08823529411764708\n"
     ]
    }
   ],
   "source": [
    "lbd = 10**-3\n",
    "maxiter=100\n",
    "model = LinearLogisticRegression()\n",
    "labs = model.train(DTR, LTR, lbd, maxiter).predict(DTE, labels=True)\n",
    "print('Error rate: ', 1 - (sum(labs == LTE) / len(LTE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70261e-33fd-45bf-9b14-e9a5286e912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbd = 10**-3\n",
    "maxiter=100\n",
    "model = QuadraticLogisticRegression()\n",
    "labs = model.train(DTR, LTR, lbd, maxiter).predict(DTE, labels=True)\n",
    "print('Error rate: ', 1 - (sum(labs == LTE) / len(LTE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71970e-41b9-4138-8d69-f8d121e25682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
